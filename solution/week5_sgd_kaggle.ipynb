{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"https://habrastorage.org/web/677/8e1/337/6778e1337c3d4b159d7e99df94227cb2.jpg\"/>\n",
    "## Специализация \"Машинное обучение и анализ данных\"\n",
    "<center>Автор материала: программист-исследователь Mail.Ru Group, старший преподаватель Факультета Компьютерных Наук ВШЭ [Юрий Кашницкий](https://yorko.github.io/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Capstone проект №1 <br> Идентификация пользователей по посещенным веб-страницам\n",
    "<img src='http://i.istockimg.com/file_thumbview_approve/21546327/5/stock-illustration-21546327-identification-de-l-utilisateur.jpg'>\n",
    "\n",
    "# <center>Неделя 5.  Соревнование Kaggle \"Catch Me If You Can\"\n",
    "\n",
    "На этой неделе мы вспомним про концепцию стохастического градиентного спуска и опробуем классификатор Scikit-learn SGDClassifier, который работает намного быстрее на больших выборках, чем алгоритмы, которые мы тестировали на 4 неделе. Также мы познакомимся с данными [соревнования](https://inclass.kaggle.com/c/catch-me-if-you-can-intruder-detection-through-webpage-session-tracking2) Kaggle по идентификации пользователей и сделаем в нем первые посылки. По итогам этой недели дополнительные баллы получат те, кто попадет в топ-30 публичного лидерборда соревнования.\n",
    "\n",
    "**В этой части проекта Вам могут быть полезны видеозаписи следующих лекций курса \"Обучение на размеченных данных\":**\n",
    "   - [Стохатический градиентный спуск](https://www.coursera.org/learn/supervised-learning/lecture/xRY50/stokhastichieskii-ghradiientnyi-spusk)\n",
    "   - [Линейные модели. Sklearn.linear_model. Классификация](https://www.coursera.org/learn/supervised-learning/lecture/EBg9t/linieinyie-modieli-sklearn-linear-model-klassifikatsiia)\n",
    "   \n",
    "**Также рекомендуется вернуться и просмотреть [задание](https://www.coursera.org/learn/supervised-learning/programming/t2Idc/linieinaia-rieghriessiia-i-stokhastichieskii-ghradiientnyi-spusk) \"Линейная регрессия и стохастический градиентный спуск\" 1 недели 2 курса специализации.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание\n",
    "1. Заполните код в этой тетрадке \n",
    "2. Если вы проходите специализацию Яндеса и МФТИ, пошлите тетрадку в соответствующем Peer Review. <br> Если вы проходите курс ODS, выберите ответы в [веб-форме](https://docs.google.com/forms/d/1pLsegkAICL9PzOLyAeH9DmDOBfktte0l8JW75uWcTng). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Решение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дальше будет много попыток получить лучшую метрику, но чтобы проверяющему не читать это всё сделаю краткую навигацию по основным пунктам программы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "бенчмарки:\n",
    "* **A3 strong baseline (20 credits)** -- 0.95965\n",
    "* **A3 strong baseline (10 credits)** -- 0.95343\n",
    "* **Logit Tf-Idf 6 features** -- 0.95216\n",
    "* **<font color='red'>Итоговый результат (n-surkov)</font>** -- 0.94171\n",
    "* **Logit +3 features** и **A3 baseline 2** -- 0.92784\n",
    "* **CountVectorizer-logit-3feat** -- 0.92692\n",
    "* **CountVectorizer-logit-3feat** -- 0.92692\n",
    "* **SGDClassifier** -- 0.91273"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Критерии оценки работы:**\n",
    "* Правильные ли получились размерности матриц в п. 1? ([ответ](#answer1))\n",
    "* Правильным ли получилось значения ROC AUC в п. 2? ([ответ](#answer2))\n",
    "* Побит ли бенчмарк \"sgd_logit_benchmark.csv\" на публичной части рейтинга в соревновании Kaggle? ([лучший результат](#cv+2fext_result) **0.94171**)\n",
    "* Побит ли бенчмарк \"Logit +3 features\" на публичной части рейтинга в соревновании Kaggle? ([лучший результат](#cv+2fext_result) **0.94171**)\n",
    "\n",
    "**ник на kaggle n-surkov**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Навигация по решению**\n",
    "* [Подготовка данных](#data_loading)\n",
    "* [Применение CountVectorizer](#count_vectorizer) ([полученный результат](#count_vectirizer_result))\n",
    "* [Добавление первой фичи](#cv+1fext) ([полученный результат](#cv+1fext_result))\n",
    "* [Добавление второй фичи](#cv+2fext) ([полученный результат](#cv+2fext_result))\n",
    "* [Дальнейшее неудачное исследование](#other) в которое вошли попытки добавить посещаемость некоторых сайтов, продолжительность сессии и настроить параметры слассификаторов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "# отключим всякие предупреждения Anaconda\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegressionCV, LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from collections import Counter\n",
    "import scipy.sparse\n",
    "from sklearn.svm import LinearSVC\n",
    "# полдключаем библиотеку проекта\n",
    "import sys\n",
    "sys.path.insert(1, './../src/')\n",
    "from preprocessing import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Считаем данные [соревнования](https://inclass.kaggle.com/c/catch-me-if-you-can-intruder-detection-through-webpage-session-tracking2) в DataFrame train_df и test_df (обучающая и тестовая выборки).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Поменяйте на свой путь к данным\n",
    "PATH_TO_DATA = os.path.join('.', '..', 'data', 'kaggle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(os.path.join(PATH_TO_DATA, 'train_sessions.csv'),\n",
    "                       index_col='session_id')\n",
    "test_df = pd.read_csv(os.path.join(PATH_TO_DATA, 'test_sessions.csv'),\n",
    "                      index_col='session_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site1</th>\n",
       "      <th>time1</th>\n",
       "      <th>site2</th>\n",
       "      <th>time2</th>\n",
       "      <th>site3</th>\n",
       "      <th>time3</th>\n",
       "      <th>site4</th>\n",
       "      <th>time4</th>\n",
       "      <th>site5</th>\n",
       "      <th>time5</th>\n",
       "      <th>...</th>\n",
       "      <th>time6</th>\n",
       "      <th>site7</th>\n",
       "      <th>time7</th>\n",
       "      <th>site8</th>\n",
       "      <th>time8</th>\n",
       "      <th>site9</th>\n",
       "      <th>time9</th>\n",
       "      <th>site10</th>\n",
       "      <th>time10</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>session_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>718</td>\n",
       "      <td>2014-02-20 10:02:45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>890</td>\n",
       "      <td>2014-02-22 11:19:50</td>\n",
       "      <td>941.0</td>\n",
       "      <td>2014-02-22 11:19:50</td>\n",
       "      <td>3847.0</td>\n",
       "      <td>2014-02-22 11:19:51</td>\n",
       "      <td>941.0</td>\n",
       "      <td>2014-02-22 11:19:51</td>\n",
       "      <td>942.0</td>\n",
       "      <td>2014-02-22 11:19:51</td>\n",
       "      <td>...</td>\n",
       "      <td>2014-02-22 11:19:51</td>\n",
       "      <td>3847.0</td>\n",
       "      <td>2014-02-22 11:19:52</td>\n",
       "      <td>3846.0</td>\n",
       "      <td>2014-02-22 11:19:52</td>\n",
       "      <td>1516.0</td>\n",
       "      <td>2014-02-22 11:20:15</td>\n",
       "      <td>1518.0</td>\n",
       "      <td>2014-02-22 11:20:16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14769</td>\n",
       "      <td>2013-12-16 16:40:17</td>\n",
       "      <td>39.0</td>\n",
       "      <td>2013-12-16 16:40:18</td>\n",
       "      <td>14768.0</td>\n",
       "      <td>2013-12-16 16:40:19</td>\n",
       "      <td>14769.0</td>\n",
       "      <td>2013-12-16 16:40:19</td>\n",
       "      <td>37.0</td>\n",
       "      <td>2013-12-16 16:40:19</td>\n",
       "      <td>...</td>\n",
       "      <td>2013-12-16 16:40:19</td>\n",
       "      <td>14768.0</td>\n",
       "      <td>2013-12-16 16:40:20</td>\n",
       "      <td>14768.0</td>\n",
       "      <td>2013-12-16 16:40:21</td>\n",
       "      <td>14768.0</td>\n",
       "      <td>2013-12-16 16:40:22</td>\n",
       "      <td>14768.0</td>\n",
       "      <td>2013-12-16 16:40:24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>782</td>\n",
       "      <td>2014-03-28 10:52:12</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-03-28 10:52:42</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-03-28 10:53:12</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-03-28 10:53:42</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-03-28 10:54:12</td>\n",
       "      <td>...</td>\n",
       "      <td>2014-03-28 10:54:42</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-03-28 10:55:12</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-03-28 10:55:42</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-03-28 10:56:12</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-03-28 10:56:42</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>22</td>\n",
       "      <td>2014-02-28 10:53:05</td>\n",
       "      <td>177.0</td>\n",
       "      <td>2014-02-28 10:55:22</td>\n",
       "      <td>175.0</td>\n",
       "      <td>2014-02-28 10:55:22</td>\n",
       "      <td>178.0</td>\n",
       "      <td>2014-02-28 10:55:23</td>\n",
       "      <td>177.0</td>\n",
       "      <td>2014-02-28 10:55:23</td>\n",
       "      <td>...</td>\n",
       "      <td>2014-02-28 10:55:59</td>\n",
       "      <td>175.0</td>\n",
       "      <td>2014-02-28 10:55:59</td>\n",
       "      <td>177.0</td>\n",
       "      <td>2014-02-28 10:55:59</td>\n",
       "      <td>177.0</td>\n",
       "      <td>2014-02-28 10:57:06</td>\n",
       "      <td>178.0</td>\n",
       "      <td>2014-02-28 10:57:11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            site1                time1  site2                time2    site3  \\\n",
       "session_id                                                                    \n",
       "1             718  2014-02-20 10:02:45    NaN                  NaN      NaN   \n",
       "2             890  2014-02-22 11:19:50  941.0  2014-02-22 11:19:50   3847.0   \n",
       "3           14769  2013-12-16 16:40:17   39.0  2013-12-16 16:40:18  14768.0   \n",
       "4             782  2014-03-28 10:52:12  782.0  2014-03-28 10:52:42    782.0   \n",
       "5              22  2014-02-28 10:53:05  177.0  2014-02-28 10:55:22    175.0   \n",
       "\n",
       "                          time3    site4                time4  site5  \\\n",
       "session_id                                                             \n",
       "1                           NaN      NaN                  NaN    NaN   \n",
       "2           2014-02-22 11:19:51    941.0  2014-02-22 11:19:51  942.0   \n",
       "3           2013-12-16 16:40:19  14769.0  2013-12-16 16:40:19   37.0   \n",
       "4           2014-03-28 10:53:12    782.0  2014-03-28 10:53:42  782.0   \n",
       "5           2014-02-28 10:55:22    178.0  2014-02-28 10:55:23  177.0   \n",
       "\n",
       "                          time5  ...                time6    site7  \\\n",
       "session_id                       ...                                 \n",
       "1                           NaN  ...                  NaN      NaN   \n",
       "2           2014-02-22 11:19:51  ...  2014-02-22 11:19:51   3847.0   \n",
       "3           2013-12-16 16:40:19  ...  2013-12-16 16:40:19  14768.0   \n",
       "4           2014-03-28 10:54:12  ...  2014-03-28 10:54:42    782.0   \n",
       "5           2014-02-28 10:55:23  ...  2014-02-28 10:55:59    175.0   \n",
       "\n",
       "                          time7    site8                time8    site9  \\\n",
       "session_id                                                               \n",
       "1                           NaN      NaN                  NaN      NaN   \n",
       "2           2014-02-22 11:19:52   3846.0  2014-02-22 11:19:52   1516.0   \n",
       "3           2013-12-16 16:40:20  14768.0  2013-12-16 16:40:21  14768.0   \n",
       "4           2014-03-28 10:55:12    782.0  2014-03-28 10:55:42    782.0   \n",
       "5           2014-02-28 10:55:59    177.0  2014-02-28 10:55:59    177.0   \n",
       "\n",
       "                          time9   site10               time10 target  \n",
       "session_id                                                            \n",
       "1                           NaN      NaN                  NaN      0  \n",
       "2           2014-02-22 11:20:15   1518.0  2014-02-22 11:20:16      0  \n",
       "3           2013-12-16 16:40:22  14768.0  2013-12-16 16:40:24      0  \n",
       "4           2014-03-28 10:56:12    782.0  2014-03-28 10:56:42      0  \n",
       "5           2014-02-28 10:57:06    178.0  2014-02-28 10:57:11      0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Объединим обучающую и тестовую выборки – это понадобится, чтоб вместе потом привести их к разреженному формату.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_df = pd.concat([train_df, test_df])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В обучающей выборке видим следующие признаки:\n",
    "    - site1 – индекс первого посещенного сайта в сессии\n",
    "    - time1 – время посещения первого сайта в сессии\n",
    "    - ...\n",
    "    - site10 – индекс 10-го посещенного сайта в сессии\n",
    "    - time10 – время посещения 10-го сайта в сессии\n",
    "    - user_id – ID пользователя\n",
    "    \n",
    "Сессии пользователей выделены таким образом, что они не могут быть длинее получаса или 10 сайтов. То есть сессия считается оконченной либо когда пользователь посетил 10 сайтов подряд, либо когда сессия заняла по времени более 30 минут. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Посмотрим на статистику признаков.**\n",
    "\n",
    "Пропуски возникают там, где сессии короткие (менее 10 сайтов). Скажем, если человек 1 января 2015 года посетил *vk.com* в 20:01, потом *yandex.ru* в 20:29, затем *google.com* в 20:33, то первая его сессия будет состоять только из двух сайтов (site1 – ID сайта *vk.com*, time1 – 2015-01-01 20:01:00, site2 – ID сайта  *yandex.ru*, time2 – 2015-01-01 20:29:00, остальные признаки – NaN), а начиная с *google.com* пойдет новая сессия, потому что уже прошло более 30 минут с момента посещения *vk.com*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 253561 entries, 1 to 253561\n",
      "Data columns (total 21 columns):\n",
      "site1     253561 non-null int64\n",
      "time1     253561 non-null object\n",
      "site2     250098 non-null float64\n",
      "time2     250098 non-null object\n",
      "site3     246919 non-null float64\n",
      "time3     246919 non-null object\n",
      "site4     244321 non-null float64\n",
      "time4     244321 non-null object\n",
      "site5     241829 non-null float64\n",
      "time5     241829 non-null object\n",
      "site6     239495 non-null float64\n",
      "time6     239495 non-null object\n",
      "site7     237297 non-null float64\n",
      "time7     237297 non-null object\n",
      "site8     235224 non-null float64\n",
      "time8     235224 non-null object\n",
      "site9     233084 non-null float64\n",
      "time9     233084 non-null object\n",
      "site10    231052 non-null float64\n",
      "time10    231052 non-null object\n",
      "target    253561 non-null int64\n",
      "dtypes: float64(9), int64(2), object(10)\n",
      "memory usage: 42.6+ MB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site1</th>\n",
       "      <th>time1</th>\n",
       "      <th>site2</th>\n",
       "      <th>time2</th>\n",
       "      <th>site3</th>\n",
       "      <th>time3</th>\n",
       "      <th>site4</th>\n",
       "      <th>time4</th>\n",
       "      <th>site5</th>\n",
       "      <th>time5</th>\n",
       "      <th>site6</th>\n",
       "      <th>time6</th>\n",
       "      <th>site7</th>\n",
       "      <th>time7</th>\n",
       "      <th>site8</th>\n",
       "      <th>time8</th>\n",
       "      <th>site9</th>\n",
       "      <th>time9</th>\n",
       "      <th>site10</th>\n",
       "      <th>time10</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>session_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29</td>\n",
       "      <td>2014-10-04 11:19:53</td>\n",
       "      <td>35.0</td>\n",
       "      <td>2014-10-04 11:19:53</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2014-10-04 11:19:54</td>\n",
       "      <td>321.0</td>\n",
       "      <td>2014-10-04 11:19:54</td>\n",
       "      <td>23.0</td>\n",
       "      <td>2014-10-04 11:19:54</td>\n",
       "      <td>2211.0</td>\n",
       "      <td>2014-10-04 11:19:54</td>\n",
       "      <td>6730.0</td>\n",
       "      <td>2014-10-04 11:19:54</td>\n",
       "      <td>21.0</td>\n",
       "      <td>2014-10-04 11:19:54</td>\n",
       "      <td>44582.0</td>\n",
       "      <td>2014-10-04 11:20:00</td>\n",
       "      <td>15336.0</td>\n",
       "      <td>2014-10-04 11:20:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>782</td>\n",
       "      <td>2014-07-03 11:00:28</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-07-03 11:00:53</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-07-03 11:00:58</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-07-03 11:01:06</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-07-03 11:01:09</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-07-03 11:01:10</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-07-03 11:01:23</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-07-03 11:01:29</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-07-03 11:01:30</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-07-03 11:01:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>55</td>\n",
       "      <td>2014-12-05 15:55:12</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2014-12-05 15:55:13</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2014-12-05 15:55:14</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2014-12-05 15:56:15</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2014-12-05 15:56:16</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2014-12-05 15:56:17</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2014-12-05 15:56:18</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2014-12-05 15:56:19</td>\n",
       "      <td>1445.0</td>\n",
       "      <td>2014-12-05 15:56:33</td>\n",
       "      <td>1445.0</td>\n",
       "      <td>2014-12-05 15:56:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1023</td>\n",
       "      <td>2014-11-04 10:03:19</td>\n",
       "      <td>1022.0</td>\n",
       "      <td>2014-11-04 10:03:19</td>\n",
       "      <td>50.0</td>\n",
       "      <td>2014-11-04 10:03:20</td>\n",
       "      <td>222.0</td>\n",
       "      <td>2014-11-04 10:03:21</td>\n",
       "      <td>202.0</td>\n",
       "      <td>2014-11-04 10:03:21</td>\n",
       "      <td>3374.0</td>\n",
       "      <td>2014-11-04 10:03:22</td>\n",
       "      <td>50.0</td>\n",
       "      <td>2014-11-04 10:03:22</td>\n",
       "      <td>48.0</td>\n",
       "      <td>2014-11-04 10:03:22</td>\n",
       "      <td>48.0</td>\n",
       "      <td>2014-11-04 10:03:23</td>\n",
       "      <td>3374.0</td>\n",
       "      <td>2014-11-04 10:03:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>301</td>\n",
       "      <td>2014-05-16 15:05:31</td>\n",
       "      <td>301.0</td>\n",
       "      <td>2014-05-16 15:05:32</td>\n",
       "      <td>301.0</td>\n",
       "      <td>2014-05-16 15:05:33</td>\n",
       "      <td>66.0</td>\n",
       "      <td>2014-05-16 15:05:39</td>\n",
       "      <td>67.0</td>\n",
       "      <td>2014-05-16 15:05:40</td>\n",
       "      <td>69.0</td>\n",
       "      <td>2014-05-16 15:05:40</td>\n",
       "      <td>70.0</td>\n",
       "      <td>2014-05-16 15:05:40</td>\n",
       "      <td>68.0</td>\n",
       "      <td>2014-05-16 15:05:40</td>\n",
       "      <td>71.0</td>\n",
       "      <td>2014-05-16 15:05:40</td>\n",
       "      <td>167.0</td>\n",
       "      <td>2014-05-16 15:05:44</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            site1                time1   site2                time2  site3  \\\n",
       "session_id                                                                   \n",
       "1              29  2014-10-04 11:19:53    35.0  2014-10-04 11:19:53   22.0   \n",
       "2             782  2014-07-03 11:00:28   782.0  2014-07-03 11:00:53  782.0   \n",
       "3              55  2014-12-05 15:55:12    55.0  2014-12-05 15:55:13   55.0   \n",
       "4            1023  2014-11-04 10:03:19  1022.0  2014-11-04 10:03:19   50.0   \n",
       "5             301  2014-05-16 15:05:31   301.0  2014-05-16 15:05:32  301.0   \n",
       "\n",
       "                          time3  site4                time4  site5  \\\n",
       "session_id                                                           \n",
       "1           2014-10-04 11:19:54  321.0  2014-10-04 11:19:54   23.0   \n",
       "2           2014-07-03 11:00:58  782.0  2014-07-03 11:01:06  782.0   \n",
       "3           2014-12-05 15:55:14   55.0  2014-12-05 15:56:15   55.0   \n",
       "4           2014-11-04 10:03:20  222.0  2014-11-04 10:03:21  202.0   \n",
       "5           2014-05-16 15:05:33   66.0  2014-05-16 15:05:39   67.0   \n",
       "\n",
       "                          time5   site6                time6   site7  \\\n",
       "session_id                                                             \n",
       "1           2014-10-04 11:19:54  2211.0  2014-10-04 11:19:54  6730.0   \n",
       "2           2014-07-03 11:01:09   782.0  2014-07-03 11:01:10   782.0   \n",
       "3           2014-12-05 15:56:16    55.0  2014-12-05 15:56:17    55.0   \n",
       "4           2014-11-04 10:03:21  3374.0  2014-11-04 10:03:22    50.0   \n",
       "5           2014-05-16 15:05:40    69.0  2014-05-16 15:05:40    70.0   \n",
       "\n",
       "                          time7  site8                time8    site9  \\\n",
       "session_id                                                             \n",
       "1           2014-10-04 11:19:54   21.0  2014-10-04 11:19:54  44582.0   \n",
       "2           2014-07-03 11:01:23  782.0  2014-07-03 11:01:29    782.0   \n",
       "3           2014-12-05 15:56:18   55.0  2014-12-05 15:56:19   1445.0   \n",
       "4           2014-11-04 10:03:22   48.0  2014-11-04 10:03:22     48.0   \n",
       "5           2014-05-16 15:05:40   68.0  2014-05-16 15:05:40     71.0   \n",
       "\n",
       "                          time9   site10               time10  \n",
       "session_id                                                     \n",
       "1           2014-10-04 11:20:00  15336.0  2014-10-04 11:20:00  \n",
       "2           2014-07-03 11:01:30    782.0  2014-07-03 11:01:53  \n",
       "3           2014-12-05 15:56:33   1445.0  2014-12-05 15:56:36  \n",
       "4           2014-11-04 10:03:23   3374.0  2014-11-04 10:03:23  \n",
       "5           2014-05-16 15:05:40    167.0  2014-05-16 15:05:44  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 82797 entries, 1 to 82797\n",
      "Data columns (total 20 columns):\n",
      "site1     82797 non-null int64\n",
      "time1     82797 non-null object\n",
      "site2     81308 non-null float64\n",
      "time2     81308 non-null object\n",
      "site3     80075 non-null float64\n",
      "time3     80075 non-null object\n",
      "site4     79182 non-null float64\n",
      "time4     79182 non-null object\n",
      "site5     78341 non-null float64\n",
      "time5     78341 non-null object\n",
      "site6     77566 non-null float64\n",
      "time6     77566 non-null object\n",
      "site7     76840 non-null float64\n",
      "time7     76840 non-null object\n",
      "site8     76151 non-null float64\n",
      "time8     76151 non-null object\n",
      "site9     75484 non-null float64\n",
      "time9     75484 non-null object\n",
      "site10    74806 non-null float64\n",
      "time10    74806 non-null object\n",
      "dtypes: float64(9), int64(1), object(10)\n",
      "memory usage: 13.3+ MB\n"
     ]
    }
   ],
   "source": [
    "test_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**В обучающей выборке – 2297 сессий одного пользователя (Alice) и 251264 сессий – других пользователей, не Элис. Дисбаланс классов очень сильный, и смотреть на долю верных ответов (accuracy) непоказательно.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    251264\n",
       "1      2297\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Пока для прогноза будем использовать только индексы посещенных сайтов. Индексы нумеровались с 1, так что заменим пропуски на нули.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_df_sites = train_test_df[['site%d' % i for i in range(1, 11)]].fillna(0).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site1</th>\n",
       "      <th>site2</th>\n",
       "      <th>site3</th>\n",
       "      <th>site4</th>\n",
       "      <th>site5</th>\n",
       "      <th>site6</th>\n",
       "      <th>site7</th>\n",
       "      <th>site8</th>\n",
       "      <th>site9</th>\n",
       "      <th>site10</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>session_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>718</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>890</td>\n",
       "      <td>941</td>\n",
       "      <td>3847</td>\n",
       "      <td>941</td>\n",
       "      <td>942</td>\n",
       "      <td>3846</td>\n",
       "      <td>3847</td>\n",
       "      <td>3846</td>\n",
       "      <td>1516</td>\n",
       "      <td>1518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14769</td>\n",
       "      <td>39</td>\n",
       "      <td>14768</td>\n",
       "      <td>14769</td>\n",
       "      <td>37</td>\n",
       "      <td>39</td>\n",
       "      <td>14768</td>\n",
       "      <td>14768</td>\n",
       "      <td>14768</td>\n",
       "      <td>14768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>782</td>\n",
       "      <td>782</td>\n",
       "      <td>782</td>\n",
       "      <td>782</td>\n",
       "      <td>782</td>\n",
       "      <td>782</td>\n",
       "      <td>782</td>\n",
       "      <td>782</td>\n",
       "      <td>782</td>\n",
       "      <td>782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>22</td>\n",
       "      <td>177</td>\n",
       "      <td>175</td>\n",
       "      <td>178</td>\n",
       "      <td>177</td>\n",
       "      <td>178</td>\n",
       "      <td>175</td>\n",
       "      <td>177</td>\n",
       "      <td>177</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>570</td>\n",
       "      <td>21</td>\n",
       "      <td>570</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>803</td>\n",
       "      <td>23</td>\n",
       "      <td>5956</td>\n",
       "      <td>17513</td>\n",
       "      <td>37</td>\n",
       "      <td>21</td>\n",
       "      <td>803</td>\n",
       "      <td>17514</td>\n",
       "      <td>17514</td>\n",
       "      <td>17514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>22</td>\n",
       "      <td>21</td>\n",
       "      <td>29</td>\n",
       "      <td>5041</td>\n",
       "      <td>14422</td>\n",
       "      <td>23</td>\n",
       "      <td>21</td>\n",
       "      <td>5041</td>\n",
       "      <td>14421</td>\n",
       "      <td>14421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>668</td>\n",
       "      <td>940</td>\n",
       "      <td>942</td>\n",
       "      <td>941</td>\n",
       "      <td>941</td>\n",
       "      <td>942</td>\n",
       "      <td>940</td>\n",
       "      <td>23</td>\n",
       "      <td>21</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3700</td>\n",
       "      <td>229</td>\n",
       "      <td>570</td>\n",
       "      <td>21</td>\n",
       "      <td>229</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>2336</td>\n",
       "      <td>2044</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            site1  site2  site3  site4  site5  site6  site7  site8  site9  \\\n",
       "session_id                                                                  \n",
       "1             718      0      0      0      0      0      0      0      0   \n",
       "2             890    941   3847    941    942   3846   3847   3846   1516   \n",
       "3           14769     39  14768  14769     37     39  14768  14768  14768   \n",
       "4             782    782    782    782    782    782    782    782    782   \n",
       "5              22    177    175    178    177    178    175    177    177   \n",
       "6             570     21    570     21     21      0      0      0      0   \n",
       "7             803     23   5956  17513     37     21    803  17514  17514   \n",
       "8              22     21     29   5041  14422     23     21   5041  14421   \n",
       "9             668    940    942    941    941    942    940     23     21   \n",
       "10           3700    229    570     21    229     21     21     21   2336   \n",
       "\n",
       "            site10  \n",
       "session_id          \n",
       "1                0  \n",
       "2             1518  \n",
       "3            14768  \n",
       "4              782  \n",
       "5              178  \n",
       "6                0  \n",
       "7            17514  \n",
       "8            14421  \n",
       "9               22  \n",
       "10            2044  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_df_sites.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Создайте разреженные матрицы *X_train_sparse* и *X_test_sparse* аналогично тому, как мы это делали ранее. Используйте объединенную матрицу *train_test_df_sites*, потом разделите обратно на обучающую и тестовую части.**\n",
    "\n",
    "Обратите внимание на то, что в  сессиях меньше 10 сайтов  у нас остались нули, так что первый признак (сколько раз попался 0) по смыслу отличен от остальных (сколько раз попался сайт с индексом $i$). Поэтому первый столбец разреженной матрицы надо будет удалить.\n",
    "\n",
    "**Выделите в отдельный вектор *y* ответы на обучающей выборке.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(PATH_TO_DATA, 'site_dic.pkl'), 'rb') as fo:\n",
    "    site_dic = pickle.load(fo)\n",
    "site_freq = dict([(key, (val, )) for key, val in site_dic.items()])\n",
    "with open(os.path.join(PATH_TO_DATA, 'site_freq.pkl'), 'wb') as fo:\n",
    "    pickle.dump(site_freq, fo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_sparse = make_csr_matrix(train_test_df_sites.to_numpy(), site_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sparse = train_test_sparse[:253561, :]\n",
    "X_test_sparse = train_test_sparse[253561:, :]\n",
    "y = train_df.target.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color='red'>Вопрос 1. </font> Выведите размерности матриц *X_train_sparse* и *X_test_sparse* – 4 числа на одной строке через пробел: число строк и столбцов матрицы *X_train_sparse*, затем число строк и столбцов матрицы *X_test_sparse*.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_sparce shape is (253561, 48371) according to train_df shape = (253561, 21)\n",
      "X_test_sparce shape is (82797, 48371) according to test_df shape = (82797, 20)\n",
      "253561 48371 82797 48371\n"
     ]
    }
   ],
   "source": [
    "print('X_train_sparce shape is {} according to train_df shape = {}'.format(X_train_sparse.shape, train_df.shape))\n",
    "print('X_test_sparce shape is {} according to test_df shape = {}'.format(X_test_sparse.shape, test_df.shape))\n",
    "print(X_train_sparse.shape[0], X_train_sparse.shape[1], X_test_sparse.shape[0], X_test_sparse.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='answer1'></a>\n",
    "**Итого получаем число строк и столбцов матрицы X_train_sparse, затем число строк и столбцов матрицы X_test_sparse:**\n",
    "\n",
    "<font color='red'>**253561 48371 82797 48371**</font>\n",
    "\n",
    "[К решению](#Решение)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Сохраним в pickle-файлы объекты *X_train_sparse*, *X_test_sparse* и *y* (последний – в файл *kaggle_data/train_target.pkl*).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(PATH_TO_DATA, 'X_train_sparse.pkl'), 'wb') as X_train_sparse_pkl:\n",
    "    pickle.dump(X_train_sparse, X_train_sparse_pkl, protocol=2)\n",
    "with open(os.path.join(PATH_TO_DATA, 'X_test_sparse.pkl'), 'wb') as X_test_sparse_pkl:\n",
    "    pickle.dump(X_test_sparse, X_test_sparse_pkl, protocol=2)\n",
    "with open(os.path.join(PATH_TO_DATA, 'train_target.pkl'), 'wb') as train_target_pkl:\n",
    "    pickle.dump(y, train_target_pkl, protocol=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Разобьем обучающую выборку на 2 части в пропорции 7/3, причем не перемешивая. Исходные данные упорядочены по времени, тестовая выборка по времени четко отделена от обучающей, это же соблюдем и здесь.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_share = int(.7 * X_train_sparse.shape[0])\n",
    "X_train, y_train = X_train_sparse[:train_share, :], y[:train_share]\n",
    "X_valid, y_valid  = X_train_sparse[train_share:, :], y[train_share:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Создайте объект `sklearn.linear_model.SGDClassifier` с логистической функцией потерь и параметром *random_state*=17. Остальные параметры оставьте по умолчанию, разве что *n_jobs*=-1 никогда не помешает. Обучите  модель на выборке `(X_train, y_train)`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 410 ms, sys: 156 ms, total: 566 ms\n",
      "Wall time: 321 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "              early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
       "              l1_ratio=0.15, learning_rate='optimal', loss='log', max_iter=1000,\n",
       "              n_iter_no_change=5, n_jobs=-1, penalty='l2', power_t=0.5,\n",
       "              random_state=17, shuffle=True, tol=0.001, validation_fraction=0.1,\n",
       "              verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "sgd_logit = SGDClassifier(loss='log', random_state=17, n_jobs=-1)\n",
    "sgd_logit.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Сделайте прогноз в виде предсказанных вероятностей того, что это сессия Элис, на отложенной выборке *(X_valid, y_valid)*.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_valid_pred_proba = sgd_logit.predict_proba(X_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color='red'>Вопрос 2. </font> Посчитайте ROC AUC логистической регрессии, обученной с помощью стохастического градиентного спуска, на отложенной выборке. Округлите до 3 знаков после разделителя.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc = roc_auc_score(y_valid, logit_valid_pred_proba[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9336212361738347\n"
     ]
    }
   ],
   "source": [
    "print(roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='answer2'></a>\n",
    "**ROC AUC логистической регрессии**\n",
    "\n",
    "<font color='red'>**0.934**</font>\n",
    "\n",
    "[К решению](#Решение)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Сделайте прогноз в виде предсказанных вероятностей отнесения к классу 1 для тестовой выборки с помощью той же *sgd_logit*, обученной уже на всей обучающей выборке (а не на 70%).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 499 ms, sys: 179 ms, total: 678 ms\n",
      "Wall time: 430 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sgd_logit.fit(X_train_sparse, y)\n",
    "logit_test_pred_proba = sgd_logit.predict_proba(X_test_sparse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Запишите ответы в файл и сделайте посылку на Kaggle. Дайте своей команде (из одного человека) на Kaggle говорящее название – по шаблону \"[YDF & MIPT] Coursera_Username\", чтоб можно было легко идентифицировать Вашу посылку на [лидерборде](https://inclass.kaggle.com/c/catch-me-if-you-can-intruder-detection-through-webpage-session-tracking2/leaderboard/public).**\n",
    "\n",
    "**Результат, который мы только что получили, соответствует бейзлайну \"SGDCLassifer\" на лидерборде, задача на эту неделю – как минимум его побить.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_submission_file(predicted_labels, out_file,\n",
    "                             target='target', index_label=\"session_id\"):\n",
    "    # turn predictions into data frame and save as csv file\n",
    "    predicted_df = pd.DataFrame(predicted_labels,\n",
    "                                index = np.arange(1, predicted_labels.shape[0] + 1),\n",
    "                                columns=[target])\n",
    "    predicted_df.to_csv(out_file, index_label=index_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_to_submission_file(logit_test_pred_proba[:, 1], os.path.join('answers', 'SGD_first_submission.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получил Score = 0.91646"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='data_loading'></a>\n",
    "## Подготовка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[К решению](#Решение)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее была собрана обучающая выборка из 150 случайных пользователей по данным соревнования. Загрузим её."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_cols = ['time%d' % i for i in range(1, 11)]\n",
    "train_subdf = pd.read_csv(os.path.join(PATH_TO_DATA, 'train_sessions_u150.csv'),\n",
    "                          index_col='session_id',\n",
    "                          parse_dates=time_cols)\n",
    "\n",
    "X_train_df, X_valid_df, y_train_ss, y_valid_ss = train_test_split(train_subdf, \n",
    "                                                                  train_subdf.target, \n",
    "                                                                  test_size=0.33, shuffle=True, \n",
    "                                                                  stratify=train_subdf.target,\n",
    "                                                                  random_state=17)\n",
    "sites_cols = ['site%d' % i for i in range(1, 11)]\n",
    "X_train_ss = X_train_df[sites_cols].fillna(0).astype('int')\n",
    "X_valid_ss = X_valid_df[sites_cols].fillna(0).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(PATH_TO_DATA, 'site_freq.pkl'), 'rb') as fo:\n",
    "    site_freq = pickle.load(fo)\n",
    "X_train_sparse_ss = make_csr_matrix(X_train_ss.to_numpy(), site_freq)\n",
    "X_valid_sparse_ss = make_csr_matrix(X_valid_ss.to_numpy(), site_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И подгрузим нормально исходные данные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_cols = ['time%d' % i for i in range(1, 11)]\n",
    "train_df = pd.read_csv(os.path.join(PATH_TO_DATA, 'train_sessions.csv'),\n",
    "                          index_col='session_id',\n",
    "                          parse_dates=time_cols)\n",
    "test_df = pd.read_csv(os.path.join(PATH_TO_DATA, 'test_sessions.csv'),\n",
    "                          index_col='session_id',\n",
    "                          parse_dates=time_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получим на данной тестовой выборке метрику логистической регрессии."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC for X_sparse with SGDClassifier is 0.96812\n"
     ]
    }
   ],
   "source": [
    "sgd_logit = SGDClassifier(loss='log', random_state=17, n_jobs=-1)\n",
    "sgd_logit.fit(X_train_sparse_ss, y_train_ss)\n",
    "logit_valid_pred_proba = sgd_logit.predict_proba(X_valid_sparse_ss)\n",
    "roc_auc = roc_auc_score(y_valid_ss, logit_valid_pred_proba[:, 1])\n",
    "print('ROC AUC for X_sparse with SGDClassifier is {:.5f}'.format(roc_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метрика получилась выше, но это и не важно, нам она нужна для сравнения алгоритмов между собой."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='count_vectorizer'></a>\n",
    "## Попробуем логистическую регрессию с CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[К решению](#Решение)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sessions have 48371 unique sites\n"
     ]
    }
   ],
   "source": [
    "print('sessions have {} unique sites'.format(len(site_freq)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Примем 50к фичей и включим 1,2,3-граммы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_countvec(X_train, X_test):\n",
    "    vectorizer = CountVectorizer(ngram_range=(1, 3), max_features=50000)\n",
    "    X_train = X_train[['site%d' % i for i in range(1, 11)]].fillna(0).astype('int')\n",
    "    X_test = X_test[['site%d' % i for i in range(1, 11)]].fillna(0).astype('int')\n",
    "    \n",
    "    X_train.to_csv('X_train_text.txt', sep=' ', index=None, header=None)\n",
    "    X_test.to_csv('X_test_text.txt', sep=' ', index=None, header=None)\n",
    "    \n",
    "    with open('X_train_text.txt', 'r') as fo:\n",
    "        X_train = vectorizer.fit_transform(fo)\n",
    "    with open('X_test_text.txt', 'r') as fo:\n",
    "        X_test = vectorizer.transform(fo)\n",
    "    \n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cv, X_test_cv = prepare_data_countvec(X_train_ss, X_valid_ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC for CountVectorizer with SGDClassifier is 0.96456\n"
     ]
    }
   ],
   "source": [
    "sgd_logit = SGDClassifier(loss='log', random_state=17, n_jobs=-1)\n",
    "sgd_logit.fit(X_train_cv, y_train_ss)\n",
    "logit_valid_pred_proba = sgd_logit.predict_proba(X_test_cv)\n",
    "roc_auc = roc_auc_score(y_valid_ss, logit_valid_pred_proba[:, 1])\n",
    "print('ROC AUC for CountVectorizer with SGDClassifier is {:.5f}'.format(roc_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='count_vectorizer_result'></a>\n",
    "**Получилось практически то же самое. Правда на реальных данных метрика получилась чуть выше (0.91789), но тоже несущественно.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Попробуем логистическую регрессию с TfIdfVEctorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_tfidfvec(X_train, X_test):\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(1, 3), max_features=50000)\n",
    "    X_train = X_train[['site%d' % i for i in range(1, 11)]].fillna(0).astype('int')\n",
    "    X_test = X_test[['site%d' % i for i in range(1, 11)]].fillna(0).astype('int')\n",
    "    \n",
    "    X_train.to_csv('X_train_text.txt', sep=' ', index=None, header=None)\n",
    "    X_test.to_csv('X_test_text.txt', sep=' ', index=None, header=None)\n",
    "    \n",
    "    with open('X_train_text.txt', 'r') as fo:\n",
    "        X_train = vectorizer.fit_transform(fo)\n",
    "    with open('X_test_text.txt', 'r') as fo:\n",
    "        X_test = vectorizer.transform(fo)\n",
    "    \n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cv, X_test_cv = prepare_data_tfidfvec(X_train_ss, X_valid_ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC for TfidfVectorizer with SGDClassifier is 0.95506\n"
     ]
    }
   ],
   "source": [
    "sgd_logit = SGDClassifier(loss='log', random_state=17, n_jobs=-1)\n",
    "sgd_logit.fit(X_train_cv, y_train_ss)\n",
    "logit_valid_pred_proba = sgd_logit.predict_proba(X_test_cv)\n",
    "roc_auc = roc_auc_score(y_valid_ss, logit_valid_pred_proba[:, 1])\n",
    "print('ROC AUC for TfidfVectorizer with SGDClassifier is {:.5f}'.format(roc_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Получилось меньше, чем у CounterVectorizer, отправлять нет смысла**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Попробуем LinearCSV, с параметрами, настроенными на 4 неделе.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cv, X_test_cv = prepare_data_countvec(X_train_ss, X_valid_ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC for CountVectorizer with LogistticRegression is 0.96945\n"
     ]
    }
   ],
   "source": [
    "estimator = LogisticRegression(C=2.9050786505108603, n_jobs=-1, random_state=17)\n",
    "estimator.fit(X_train_cv, y_train_ss)\n",
    "logit_valid_pred_proba = estimator.predict_proba(X_test_cv)\n",
    "roc_auc = roc_auc_score(y_valid_ss, logit_valid_pred_proba[:, 1])\n",
    "print('ROC AUC for CountVectorizer with LogistticRegression is {:.5f}'.format(roc_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Значительных улучшений не дало, пробуем искать дальше**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='cv+1fext'></a>\n",
    "## Дополним данные временем суток начала сессии"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[К решению](#Решение)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_features(csr, X):\n",
    "    features = [\n",
    "                'daily_aсtivity', \n",
    "               ]\n",
    "    sessions = X\n",
    "    sessions['user_id'] = np.zeros((len(X), ), dtype=int)\n",
    "    X_ext = prepare_train_set_fe(sessions,\n",
    "                                 site_freq_path=os.path.join(PATH_TO_DATA, 'site_freq.pkl'),\n",
    "                                 feature_names=features)\n",
    "    \n",
    "    csr_ext = csr.copy()\n",
    "    for t in np.unique(X_ext['daily_aсtivity']):\n",
    "        new_data = (X_ext['daily_aсtivity'] == t).astype('int')\n",
    "        csr_ext = hstack([csr_ext, new_data.to_numpy().reshape(-1, 1)])\n",
    "    \n",
    "    return csr_ext\n",
    "    \n",
    "def prepare_data_ext(X_train, X_test):\n",
    "    vectorizer = CountVectorizer(ngram_range=(1, 3), max_features=50000)\n",
    "    \n",
    "    X_train_ss = X_train[['site%d' % i for i in range(1, 11)]].fillna(0).astype('int')\n",
    "    X_test_ss = X_test[['site%d' % i for i in range(1, 11)]].fillna(0).astype('int')\n",
    "    \n",
    "    X_train_ss.to_csv('X_train_text.txt', sep=' ', index=None, header=None)\n",
    "    X_test_ss.to_csv('X_test_text.txt', sep=' ', index=None, header=None)\n",
    "    \n",
    "    with open('X_train_text.txt', 'r') as fo:\n",
    "        X_train_csr = vectorizer.fit_transform(fo)\n",
    "    with open('X_test_text.txt', 'r') as fo:\n",
    "        X_test_csr = vectorizer.transform(fo)\n",
    "        \n",
    "    X_train = extend_features(X_train_csr, X_train)\n",
    "    X_test = extend_features(X_test_csr, X_test)\n",
    "    \n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cv, X_test_cv = prepare_data_ext(X_train_df, X_valid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC for CountVectorizer + 3 features with SGDClassifier is 0.98094\n"
     ]
    }
   ],
   "source": [
    "sgd_logit = SGDClassifier(loss='log', random_state=17, n_jobs=-1)\n",
    "sgd_logit.fit(X_train_cv, y_train_ss)\n",
    "logit_valid_pred_proba = sgd_logit.predict_proba(X_test_cv)\n",
    "roc_auc = roc_auc_score(y_valid_ss, logit_valid_pred_proba[:, 1])\n",
    "print('ROC AUC for CountVectorizer + 1 feature with SGDClassifier is {:.5f}'.format(roc_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Значительно лучше, попробуем на полной тестовой выборке.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cv, X_test_cv = prepare_data_ext(train_df.iloc[:, :-1], test_df.iloc[:, :-1])\n",
    "train_share = int(.7 * X_train_sparse.shape[0])\n",
    "X_train, y_train = X_train_cv.tocsr()[:train_share, :], train_df.target[:train_share]\n",
    "X_valid, y_valid  = X_train_cv.tocsr()[train_share:, :], train_df.target[train_share:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC for CountVectorizer + 1 feature with SGDClassifier is 0.96457\n"
     ]
    }
   ],
   "source": [
    "sgd_logit = SGDClassifier(loss='log', random_state=17, n_jobs=-1)\n",
    "sgd_logit.fit(X_train, y_train)\n",
    "logit_valid_pred_proba = sgd_logit.predict_proba(X_valid)\n",
    "roc_auc = roc_auc_score(y_valid, logit_valid_pred_proba[:, 1])\n",
    "print('ROC AUC for CountVectorizer + 1 feature with SGDClassifier is {:.5f}'.format(roc_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Действительно лучше, можно попробовать на реальных данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_logit = SGDClassifier(loss='log', random_state=17, n_jobs=-1)\n",
    "sgd_logit.fit(X_train_cv, train_df.target)\n",
    "logit_test_pred_proba = sgd_logit.predict_proba(X_test_cv)\n",
    "write_to_submission_file(logit_test_pred_proba[:, 1], os.path.join('answers', 'SGD_countVect_1fext.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='cv+1fext_result'></a>\n",
    "**Получил Score = 0.93744**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='cv+2fext'></a>\n",
    "## Добавление дня недели начала сессии"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[К решению](#Решение)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_features(csr, X):\n",
    "    features = [\n",
    "                'day_of_week', \n",
    "                'daily_aсtivity',\n",
    "               ]\n",
    "    sessions = X\n",
    "    sessions['user_id'] = np.zeros((len(X), ), dtype=int)\n",
    "    X_ext = prepare_train_set_fe(sessions,\n",
    "                                 site_freq_path=os.path.join(PATH_TO_DATA, 'site_freq.pkl'),\n",
    "                                 feature_names=features)\n",
    "    \n",
    "    csr_ext = csr.copy()\n",
    "    for t in np.unique(X_ext['daily_aсtivity']):\n",
    "        new_data = (X_ext['daily_aсtivity'] == t).astype('int')\n",
    "        csr_ext = hstack([csr_ext, new_data.to_numpy().reshape(-1, 1)])\n",
    "        \n",
    "    for t in np.unique(X_ext['day_of_week']):\n",
    "        new_data = (X_ext['day_of_week'] == t).astype('int')\n",
    "        csr_ext = hstack([csr_ext, new_data.to_numpy().reshape(-1, 1)])\n",
    "        \n",
    "    return csr_ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cv, X_test_cv = prepare_data_ext(X_train_df, X_valid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC for CountVectorizer + 4 features with SGDClassifier is 0.98489\n"
     ]
    }
   ],
   "source": [
    "sgd_logit = SGDClassifier(loss='log', random_state=17, n_jobs=-1)\n",
    "sgd_logit.fit(X_train_cv, y_train_ss)\n",
    "logit_valid_pred_proba = sgd_logit.predict_proba(X_test_cv)\n",
    "roc_auc = roc_auc_score(y_valid_ss, logit_valid_pred_proba[:, 1])\n",
    "print('ROC AUC for CountVectorizer + 2 features with SGDClassifier is {:.5f}'.format(roc_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ещё лучше, пробуем на полной."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cv, X_test_cv = prepare_data_ext(train_df.iloc[:, :-1], test_df.iloc[:, :-1])\n",
    "train_share = int(.7 * X_train_sparse.shape[0])\n",
    "X_train, y_train = X_train_cv.tocsr()[:train_share, :], train_df.target[:train_share]\n",
    "X_valid, y_valid  = X_train_cv.tocsr()[train_share:, :], train_df.target[train_share:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC for CountVectorizer + 4 feature with SGDClassifier is 0.96964\n"
     ]
    }
   ],
   "source": [
    "sgd_logit = SGDClassifier(loss='log', random_state=17, n_jobs=-1)\n",
    "sgd_logit.fit(X_train, y_train)\n",
    "logit_valid_pred_proba = sgd_logit.predict_proba(X_valid)\n",
    "roc_auc = roc_auc_score(y_valid, logit_valid_pred_proba[:, 1])\n",
    "print('ROC AUC for CountVectorizer + 4 feature with SGDClassifier is {:.5f}'.format(roc_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно пробовать отправить"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_logit = SGDClassifier(loss='log', random_state=17, n_jobs=-1)\n",
    "sgd_logit.fit(X_train_cv, train_df.target)\n",
    "logit_test_pred_proba = sgd_logit.predict_proba(X_test_cv)\n",
    "write_to_submission_file(logit_test_pred_proba[:, 1], os.path.join('answers', 'SGD_countVect_4fext.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='cv+2fext_result'></a>\n",
    "**Получил Score = 0.94171**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='other'></a>\n",
    "## Остальные неудачные попытки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавим продолжительность сессии в признаки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_features(csr, X):\n",
    "    features = [\n",
    "#                 '#unique_sites',\n",
    "                'session_timespan', \n",
    "#                 'start_hour',\n",
    "                'day_of_week', \n",
    "#                 'timespan_median', \n",
    "#                 'timespan_mean',\n",
    "                'daily_aсtivity', \n",
    "#                 'freq_facebook', \n",
    "#                 'timespan_youtube',\n",
    "#                 'timespan_mail', \n",
    "#                 'freq_googlevideo', \n",
    "#                 'freq_google',\n",
    "# #                 'live',\n",
    "# #                 'trouter',\n",
    "#                 'skype',\n",
    "#                 'twitter',\n",
    "               ]\n",
    "    sessions = X\n",
    "    sessions['user_id'] = np.zeros((len(X), ), dtype=int)\n",
    "    X_ext = prepare_train_set_fe(sessions,\n",
    "                                 site_freq_path=os.path.join(PATH_TO_DATA, 'site_freq.pkl'),\n",
    "                                 feature_names=features)\n",
    "    \n",
    "    csr_ext = csr.copy()\n",
    "    for t in np.unique(X_ext['daily_aсtivity']):\n",
    "        new_data = (X_ext['daily_aсtivity'] == t).astype('int')\n",
    "        csr_ext = hstack([csr_ext, new_data.to_numpy().reshape(-1, 1)])\n",
    "        \n",
    "    for t in np.unique(X_ext['day_of_week']):\n",
    "        new_data = (X_ext['day_of_week'] == t).astype('int')\n",
    "        csr_ext = hstack([csr_ext, new_data.to_numpy().reshape(-1, 1)])\n",
    "    \n",
    "    bm = [-1, 60, 5*60, 10*60, 30*60]\n",
    "    for i in range(len(bm) - 1):\n",
    "        new_data = ((X_ext['session_timespan'] > bm[i]) & \\\n",
    "                    (X_ext['session_timespan'] <= bm[i+1])).astype('int')\n",
    "        csr_ext = hstack([csr_ext, new_data.to_numpy().reshape(-1, 1)])\n",
    "        \n",
    "    return csr_ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cv, X_test_cv = prepare_data_ext(X_train_df, X_valid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC for CountVectorizer + 3 features with SGDClassifier is 0.98492\n"
     ]
    }
   ],
   "source": [
    "sgd_logit = SGDClassifier(loss='log', random_state=17, n_jobs=-1)\n",
    "sgd_logit.fit(X_train_cv, y_train_ss)\n",
    "logit_valid_pred_proba = sgd_logit.predict_proba(X_test_cv)\n",
    "roc_auc = roc_auc_score(y_valid_ss, logit_valid_pred_proba[:, 1])\n",
    "print('ROC AUC for CountVectorizer + 3 features with SGDClassifier is {:.5f}'.format(roc_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метрика получше, посмотрим, как на полной тестовой выборке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cv, X_test_cv = prepare_data_ext(train_df.iloc[:, :-1], test_df.iloc[:, :-1])\n",
    "train_share = int(.7 * len(train_df))\n",
    "X_train, y_train = X_train_cv.tocsr()[:train_share, :], train_df.target[:train_share]\n",
    "X_valid, y_valid  = X_train_cv.tocsr()[train_share:, :], train_df.target[train_share:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC for CountVectorizer + 3 feature with SGDClassifier is 0.96997\n"
     ]
    }
   ],
   "source": [
    "sgd_logit = SGDClassifier(loss='log', random_state=17, n_jobs=-1)\n",
    "sgd_logit.fit(X_train, y_train)\n",
    "logit_valid_pred_proba = sgd_logit.predict_proba(X_valid)\n",
    "roc_auc = roc_auc_score(y_valid, logit_valid_pred_proba[:, 1])\n",
    "print('ROC AUC for CountVectorizer + 3 feature with SGDClassifier is {:.5f}'.format(roc_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Действительно получшеб попробуем отправить."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_logit = SGDClassifier(loss='log', random_state=17, n_jobs=-1)\n",
    "sgd_logit.fit(X_train_cv, train_df.target)\n",
    "logit_test_pred_proba = sgd_logit.predict_proba(X_test_cv)\n",
    "write_to_submission_file(logit_test_pred_proba[:, 1], os.path.join('answers', 'SGD_countVect_3fext.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получил Score = 0.94169 -- копание на месте."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем внедрить индикацию посещений некоторых сайтов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('fpdownload2.macromedia.com', (1,)),\n",
       " ('hotmail.fr', (2,)),\n",
       " ('login.live.com', (3,)),\n",
       " ('mail.live.com', (4,)),\n",
       " ('dub122.mail.live.com', (5,)),\n",
       " ('people.directory.live.com', (6,)),\n",
       " ('secure.shared.live.com', (7,)),\n",
       " ('windowslive.tt.omtrdc.net', (8,)),\n",
       " ('cid-1bed360223325286.users.storage.live.com', (9,)),\n",
       " ('js.live.net', (10,)),\n",
       " ('go.trouter.io', (11,)),\n",
       " ('storage.live.com', (12,)),\n",
       " ('blufiles.storage.msn.com', (13,)),\n",
       " ('h.live.com', (14,)),\n",
       " ('prod.registrar.skype.com', (15,)),\n",
       " ('api.skype.com', (16,)),\n",
       " ('proxy-bay-people.directory.live.com', (17,)),\n",
       " ('p.sfx.ms', (18,)),\n",
       " ('dub122.afx.ms', (19,)),\n",
       " ('google.fr', (20,)),\n",
       " ('www.google.fr', (21,)),\n",
       " ('apis.google.com', (22,)),\n",
       " ('www.google.com', (23,)),\n",
       " ('lipsakiss.free.fr', (24,)),\n",
       " ('www.shotgun2013.fr', (25,)),\n",
       " ('ba.commentcamarche.net', (26,)),\n",
       " ('www.commentcamarche.net', (27,)),\n",
       " ('static.commentcamarche.net', (28,)),\n",
       " ('www.facebook.com', (29,)),\n",
       " ('platform.twitter.com', (30,)),\n",
       " ('static.ccm2.net', (31,)),\n",
       " ('connect.facebook.net', (32,)),\n",
       " ('static.ak.facebook.com', (33,)),\n",
       " ('r.ccm2.net', (34,)),\n",
       " ('s-static.ak.facebook.com', (35,)),\n",
       " ('static.ccmbg.com', (36,)),\n",
       " ('twitter.com', (37,)),\n",
       " ('ajax.googleapis.com', (38,)),\n",
       " ('accounts.google.com', (39,)),\n",
       " ('www.math.u-psud.fr', (40,)),\n",
       " ('kh6qrx0f8h.s.ad6media.fr', (41,)),\n",
       " ('style2.ad6.fr', (42,)),\n",
       " ('medias.lapostemobile.fr', (43,)),\n",
       " ('www.lapostemobile.fr', (44,)),\n",
       " ('www.squid-cache.org', (45,)),\n",
       " ('evsecure-ocsp.verisign.com', (46,)),\n",
       " ('evintl-ocsp.verisign.com', (47,)),\n",
       " ('ocsp.digicert.com', (48,)),\n",
       " ('gtssl-ocsp.geotrust.com', (49,)),\n",
       " ('ocsp.verisign.com', (50,)),\n",
       " ('dub128.mail.live.com', (51,)),\n",
       " ('clients1.google.com', (52,)),\n",
       " ('gtglobal-ocsp.geotrust.com', (53,)),\n",
       " ('h.bing.com', (54,)),\n",
       " ('safebrowsing-cache.google.com', (55,)),\n",
       " ('safebrowsing.clients.google.com', (56,)),\n",
       " ('sb-ssl.google.com', (57,)),\n",
       " ('dl.patchbeam.com', (58,)),\n",
       " ('www.powerarchiver.com', (59,)),\n",
       " ('bnpparisbas.net', (60,)),\n",
       " ('d1q62gfb8siqnm.cloudfront.net', (61,)),\n",
       " ('offres-speciales.bnpparibas.net', (62,)),\n",
       " ('ieonline.microsoft.com', (63,)),\n",
       " ('login.skype.com', (64,)),\n",
       " ('g.live.com', (65,)),\n",
       " ('go.microsoft.com', (66,)),\n",
       " ('windows.microsoft.com', (67,)),\n",
       " ('ajax.microsoft.com', (68,)),\n",
       " ('res2.windows.microsoft.com', (69,)),\n",
       " ('res1.windows.microsoft.com', (70,)),\n",
       " ('js.microsoft.com', (71,)),\n",
       " ('perso.telecom-paristech.fr', (72,)),\n",
       " ('ad.foxitsoftware.com', (73,)),\n",
       " ('youtube.com', (74,)),\n",
       " ('s.ytimg.com', (75,)),\n",
       " ('www.youtube.com', (76,)),\n",
       " ('i1.ytimg.com', (77,)),\n",
       " ('yt3.ggpht.com', (78,)),\n",
       " ('yt4.ggpht.com', (79,)),\n",
       " ('s.youtube.com', (80,)),\n",
       " ('r4---sn-gxo5uxg-jqbe.googlevideo.com', (81,)),\n",
       " ('r2---sn-gxo5uxg-jqbe.googlevideo.com', (82,)),\n",
       " ('www.mathworks.fr', (83,)),\n",
       " ('www.mathworks.com', (84,)),\n",
       " ('p.typekit.net', (85,)),\n",
       " ('metrics.mathworks.com', (86,)),\n",
       " ('s3.amazonaws.com', (87,)),\n",
       " ('cbks2.google.com', (88,)),\n",
       " ('perso.univ-rennes1.fr', (89,)),\n",
       " ('blogperso.univ-rennes1.fr', (90,)),\n",
       " ('www.proba.jussieu.fr', (91,)),\n",
       " ('mediamining.univ-lyon2.fr', (92,)),\n",
       " ('www.enseignement.polytechnique.fr', (93,)),\n",
       " ('hueber.thomas.free.fr', (94,)),\n",
       " ('d21utl1jejzb41.cloudfront.net', (95,)),\n",
       " ('photo3zoosk-a.akamaihd.net', (96,)),\n",
       " ('static3zoosk-a.akamaihd.net', (97,)),\n",
       " ('photo2zoosk-a.akamaihd.net', (98,)),\n",
       " ('1.gravatar.com', (99,)),\n",
       " ('aminelgareh.files.wordpress.com', (100,)),\n",
       " ('aminelgareh.wordpress.com', (101,)),\n",
       " ('s0.wp.com', (102,)),\n",
       " ('s2.wp.com', (103,)),\n",
       " ('i0.poll.fm', (104,)),\n",
       " ('s1.wp.com', (105,)),\n",
       " ('0.gravatar.com', (106,)),\n",
       " ('api.facebook.com', (107,)),\n",
       " ('polldaddy.com', (108,)),\n",
       " ('widgets.wp.com', (109,)),\n",
       " ('icdn2.digitaltrends.com', (110,)),\n",
       " ('assets.vice.com', (111,)),\n",
       " ('www.justformetoday.com', (112,)),\n",
       " ('public-api.wordpress.com', (113,)),\n",
       " ('b3447.googlecode.com', (114,)),\n",
       " ('codes-sources.commentcamarche.net', (115,)),\n",
       " ('r.ccmbg.com', (116,)),\n",
       " ('www.linkedin.com', (117,)),\n",
       " ('od.ccm2.net', (118,)),\n",
       " ('www.linternaute.com', (119,)),\n",
       " ('eea.linternaute.com', (120,)),\n",
       " ('p.ccmbg.com', (121,)),\n",
       " ('player.ooyala.com', (122,)),\n",
       " ('i-linter.linternaute.com', (123,)),\n",
       " ('www.francetvinfo.fr', (124,)),\n",
       " ('videosfr.s3.amazonaws.com', (125,)),\n",
       " ('video.journaldesfemmes.com', (126,)),\n",
       " ('opf.ooyala.com', (127,)),\n",
       " ('cdn-api.ooyala.com', (128,)),\n",
       " ('ak.c.ooyala.com', (129,)),\n",
       " ('ec.c.ooyala.com', (130,)),\n",
       " ('eulerian.canal-plus.com', (131,)),\n",
       " ('l.ooyala.com', (132,)),\n",
       " ('widgets.pinterest.com', (133,)),\n",
       " ('pings.conviva.com', (134,)),\n",
       " ('la-conjugaison.nouvelobs.com', (135,)),\n",
       " ('referentiel.dev.nouvelobs.com', (136,)),\n",
       " ('programme-tv.nouvelobs.com', (137,)),\n",
       " ('tempsreel.nouvelobs.com', (138,)),\n",
       " ('referentiel.nouvelobs.com', (139,)),\n",
       " ('pixel.alephd.com', (140,)),\n",
       " ('cimg.leguide.com', (141,)),\n",
       " ('cshoppingbox.partner.leguide.com', (142,)),\n",
       " ('api.adyoulike.com', (143,)),\n",
       " ('cdnjs.cloudflare.com', (144,)),\n",
       " ('code.jquery.com', (145,)),\n",
       " ('baf.linternaute.com', (146,)),\n",
       " ('home.edt02.net', (147,)),\n",
       " ('its.tradelab.fr', (148,)),\n",
       " ('dad.linternaute.com', (149,)),\n",
       " ('aaa.linternaute.com', (150,)),\n",
       " ('www.beead.fr', (151,)),\n",
       " ('pixel2368.everesttech.net', (152,)),\n",
       " ('www.larousse.fr', (153,)),\n",
       " ('www.conforama.fr', (154,)),\n",
       " ('x2.vindicosuite.com', (155,)),\n",
       " ('match.rtbidder.net', (156,)),\n",
       " ('ce.lijit.com', (157,)),\n",
       " ('s.kau.li', (158,)),\n",
       " ('y.one.impact-ad.jp', (159,)),\n",
       " ('admaym.com', (160,)),\n",
       " ('adventori.com', (161,)),\n",
       " ('pbs.twimg.com', (162,)),\n",
       " ('images.eveiletjeux.net', (163,)),\n",
       " ('abs.twimg.com', (164,)),\n",
       " ('img.jogtheweb.com', (165,)),\n",
       " ('www.jogtheweb.com', (166,)),\n",
       " ('www.bing.com', (167,)),\n",
       " ('dub128.afx.ms', (168,)),\n",
       " ('word-view.officeapps.live.com', (169,)),\n",
       " ('player.vimeo.com', (170,)),\n",
       " ('www.cmi.univ-mrs.fr', (171,)),\n",
       " ('ureca.recherche.univ-lille3.fr', (172,)),\n",
       " ('fr-fr.facebook.com', (173,)),\n",
       " ('atoms.scilab.org', (174,)),\n",
       " ('bits.wikimedia.org', (175,)),\n",
       " ('login.wikimedia.org', (176,)),\n",
       " ('fr.wikipedia.org', (177,)),\n",
       " ('meta.wikimedia.org', (178,)),\n",
       " ('www.scilab.org', (179,)),\n",
       " ('scilab.developpez.com', (180,)),\n",
       " ('www.developpez.com', (181,)),\n",
       " ('premium.hi-mediaserver.com', (182,)),\n",
       " ('static.francetv.fr', (183,)),\n",
       " ('motus.france2.fr', (184,)),\n",
       " ('d7v0k4dt27zlp.cloudfront.net', (185,)),\n",
       " ('fbexternal-a.akamaihd.net', (186,)),\n",
       " ('www.freefem.org', (187,)),\n",
       " ('www.hotmail.fr', (188,)),\n",
       " ('www.bnpparibas.net', (189,)),\n",
       " ('fw.dnslink.com', (190,)),\n",
       " ('live-winners.com', (191,)),\n",
       " ('njk0.hi5links.com', (192,)),\n",
       " ('go.game321.com', (193,)),\n",
       " ('passport.game321.com', (194,)),\n",
       " ('s.g321.it', (195,)),\n",
       " ('ocsp.tcs.terena.org', (196,)),\n",
       " ('ocsp.usertrust.com', (197,)),\n",
       " ('free.fr', (198,)),\n",
       " ('www.free.fr', (199,)),\n",
       " ('statsweb.proxad.net', (200,)),\n",
       " ('mobile.free.fr', (201,)),\n",
       " ('rapidssl-ocsp.geotrust.com', (202,)),\n",
       " ('evsecure-crl.verisign.com', (203,)),\n",
       " ('evintl-crl.verisign.com', (204,)),\n",
       " ('ocsp.entrust.net', (205,)),\n",
       " ('outlook.com', (206,)),\n",
       " ('www.outlook.com', (207,)),\n",
       " ('www-gm3.univ-mrs.fr', (208,)),\n",
       " ('hal.archives-ouvertes.fr', (209,)),\n",
       " ('lmrs.univ-rouen.fr', (210,)),\n",
       " ('msor.victoria.ac.nz', (211,)),\n",
       " ('uma.ensta-paristech.fr', (212,)),\n",
       " ('facebook.fr', (213,)),\n",
       " ('www.mathe-fa.de', (214,)),\n",
       " ('grapheur.cours-de-math.eu', (215,)),\n",
       " ('www.cours-de-math.eu', (216,)),\n",
       " ('hal9000.redintelligence.net', (217,)),\n",
       " ('n38.hal9000.redintelligence.net', (218,)),\n",
       " ('affiliation.service-voyages.com', (219,)),\n",
       " ('ad.publicidees.com', (220,)),\n",
       " ('medias.norauto.fr', (221,)),\n",
       " ('ocsp.thawte.com', (222,)),\n",
       " ('www.maths-et-tiques.fr', (223,)),\n",
       " ('m3.moostik.net', (224,)),\n",
       " ('sites.google.com', (225,)),\n",
       " ('www.openeering.com', (226,)),\n",
       " ('wiki.scilab.org', (227,)),\n",
       " ('www.lacl.fr', (228,)),\n",
       " ('clients1.google.fr', (229,)),\n",
       " ('www.dvdreamscape.fr', (230,)),\n",
       " ('nf.nci.org.au', (231,)),\n",
       " ('matlab.developpez.com', (232,)),\n",
       " ('sync.uuidcshmg.com', (233,)),\n",
       " ('ecx.images-amazon.com', (234,)),\n",
       " ('rcm-fr.amazon.fr', (235,)),\n",
       " ('dps.bing.com', (236,)),\n",
       " ('matlabgeeks.com', (237,)),\n",
       " ('twitter-badges.s3.amazonaws.com', (238,)),\n",
       " ('www.feedburner.com', (239,)),\n",
       " ('i2.ytimg.com', (240,)),\n",
       " ('www.cmap.polytechnique.fr', (241,)),\n",
       " ('www.developpez.net', (242,)),\n",
       " ('help.scilab.org', (243,)),\n",
       " ('math.linux-sottises.net', (244,)),\n",
       " ('mathworld.wolfram.com', (245,)),\n",
       " ('www.wolframalpha.com', (246,)),\n",
       " ('www4c.wolframalpha.com', (247,)),\n",
       " ('w.sharethis.com', (248,)),\n",
       " ('dub126.mail.live.com', (249,)),\n",
       " ('www.lmm.jussieu.fr', (250,)),\n",
       " ('www.fstm.ac.ma', (251,)),\n",
       " ('www.aldaniti.net', (252,)),\n",
       " ('ksvzyt5fp1.s.ad6media.fr', (253,)),\n",
       " ('eda.linternaute.com', (254,)),\n",
       " ('loadr.exelator.com', (255,)),\n",
       " ('broadcast.piximedia.fr', (256,)),\n",
       " ('abb.linternaute.com', (257,)),\n",
       " ('static.ebz.io', (258,)),\n",
       " ('fee.linternaute.com', (259,)),\n",
       " ('www.emailmerci.fr', (260,)),\n",
       " ('banniere.reussissonsensemble.fr', (261,)),\n",
       " ('apnx-match.dotomi.com', (262,)),\n",
       " ('www.caf.fr', (263,)),\n",
       " ('caf.fr', (264,)),\n",
       " ('wwwd.caf.fr', (265,)),\n",
       " ('cid-4da1a109922a6796.users.storage.live.com', (266,)),\n",
       " ('dub123.mail.live.com', (267,)),\n",
       " ('fr.msn.com', (268,)),\n",
       " ('h2.msn.com', (269,)),\n",
       " ('api.bing.com', (270,)),\n",
       " ('c.fr.msn.com', (271,)),\n",
       " ('g.msn.com', (272,)),\n",
       " ('cntr.adrcntr.com', (273,)),\n",
       " ('evt.adrcntr.com', (274,)),\n",
       " ('www.math.u-bordeaux1.fr', (275,)),\n",
       " ('answers.yahoo.com', (276,)),\n",
       " ('yui-s.yahooapis.com', (277,)),\n",
       " ('dmros.ysm.yahoo.com', (278,)),\n",
       " ('jeux-et-mathematiques.davalan.org', (279,)),\n",
       " ('jm.davalan.org', (280,)),\n",
       " ('jean-paul.davalan.pagesperso-orange.fr', (281,)),\n",
       " ('alain.troesch.free.fr', (282,)),\n",
       " ('www.les-mathematiques.net', (283,)),\n",
       " ('www.association-tremplin.org', (284,)),\n",
       " ('vekemans.free.fr', (285,)),\n",
       " ('www.decitre.fr', (286,)),\n",
       " ('books.google.com', (287,)),\n",
       " ('decitre.di-static.com', (288,)),\n",
       " ('media.entreelivre.com', (289,)),\n",
       " ('static.affilae.com', (290,)),\n",
       " ('halc.iadvize.com', (291,)),\n",
       " ('fstatic.iadvize.com', (292,)),\n",
       " ('jserror.newrelic.com', (293,)),\n",
       " ('www.ilemaths.net', (294,)),\n",
       " ('latex.ilemaths.net', (295,)),\n",
       " ('adq.nextag.fr', (296,)),\n",
       " ('img05.static-nextag.com', (297,)),\n",
       " ('static.lesoffrescanal.fr', (298,)),\n",
       " ('www4b.wolframalpha.com', (299,)),\n",
       " ('www.univ-bpclermont.fr', (300,)),\n",
       " ('sci.sciences.univ-bpclermont.fr', (301,)),\n",
       " ('ent.univ-bpclermont.fr', (302,)),\n",
       " ('www.mozilla.org', (303,)),\n",
       " ('geo.mozilla.org', (304,)),\n",
       " ('office14client.microsoft.com', (305,)),\n",
       " ('office.microsoft.com', (306,)),\n",
       " ('www.cnrs.fr', (307,)),\n",
       " ('www2.cnrs.fr', (308,)),\n",
       " ('cdn1-lejdd.ladmedia.fr', (309,)),\n",
       " ('www.lejdd.fr', (310,)),\n",
       " ('cdn3-lejdd.ladmedia.fr', (311,)),\n",
       " ('cdn2-lejdd.ladmedia.fr', (312,)),\n",
       " ('cdn-lejdd.ladmedia.fr', (313,)),\n",
       " ('team.newsweb.fr', (314,)),\n",
       " ('i11.twenga.com', (315,)),\n",
       " ('s0r.c4tw.net', (316,)),\n",
       " ('wtpn.twenga.fr', (317,)),\n",
       " ('rbx-cdn-tpn.c4tw.net', (318,)),\n",
       " ('rp.gwallet.com', (319,)),\n",
       " ('i10.twenga.com', (320,)),\n",
       " ('c1.adform.net', (321,)),\n",
       " ('um.simpli.fi', (322,)),\n",
       " ('rbp.mxptint.net', (323,)),\n",
       " ('rubicon-match.dotomi.com', (324,)),\n",
       " ('pr.ybp.yahoo.com', (325,)),\n",
       " ('dtm.eastbay.com', (326,)),\n",
       " ('s374.meetrics.net', (327,)),\n",
       " ('www.larecherche.fr', (328,)),\n",
       " ('static.issuu.com', (329,)),\n",
       " ('document.issuu.com', (330,)),\n",
       " ('page.issuu.com', (331,)),\n",
       " ('skin.issuu.com', (332,)),\n",
       " ('api.issuu.com', (333,)),\n",
       " ('cm.eyedemand.com', (334,)),\n",
       " ('www.dailymotion.com', (335,)),\n",
       " ('forums.futura-sciences.com', (336,)),\n",
       " ('www.futura-sciences.com', (337,)),\n",
       " ('yui.yahooapis.com', (338,)),\n",
       " ('fr.intext.adfever.com', (339,)),\n",
       " ('legacy.futura-sciences.com', (340,)),\n",
       " ('ea.numericable.fr', (341,)),\n",
       " ('www.groupon.fr', (342,)),\n",
       " ('en.wikipedia.org', (343,)),\n",
       " ('search.microsoft.com', (344,)),\n",
       " ('i.s-microsoft.com', (345,)),\n",
       " ('cs.microsoft.com', (346,)),\n",
       " ('nexus.ensighten.com', (347,)),\n",
       " ('www.microsoft.com', (348,)),\n",
       " ('c1.microsoft.com', (349,)),\n",
       " ('choices.truste.com', (350,)),\n",
       " ('www.societechimiquedefrance.fr', (351,)),\n",
       " ('assets.pinterest.com', (352,)),\n",
       " ('www.scientificamerican.com', (353,)),\n",
       " ('passets.pinterest.com', (354,)),\n",
       " ('cdns.gigya.com', (355,)),\n",
       " ('www.cea.fr', (356,)),\n",
       " ('www.sudoku-gratuit.fr', (357,)),\n",
       " ('www.1000-annonces.com', (358,)),\n",
       " ('cms.abmr.net', (359,)),\n",
       " ('ts2.mm.bing.net', (360,)),\n",
       " ('ts3.explicit.bing.net', (361,)),\n",
       " ('ts4.mm.bing.net', (362,)),\n",
       " ('ts1.mm.bing.net', (363,)),\n",
       " ('ts3.mm.bing.net', (364,)),\n",
       " ('uhuz.bplaced.net', (365,)),\n",
       " ('blog.slate.fr', (366,)),\n",
       " ('1.bp.blogspot.com', (367,)),\n",
       " ('www.lektorat.de', (368,)),\n",
       " ('www.fosbos-rosenheim.de', (369,)),\n",
       " ('www.tunisienumerique.com', (370,)),\n",
       " ('www.umoncton.ca', (371,)),\n",
       " ('www.leibniz-gymnasium.de', (372,)),\n",
       " ('www.unige.ch', (373,)),\n",
       " ('www.bestgraph.com', (374,)),\n",
       " ('www.vysokeskoly.cz', (375,)),\n",
       " ('sienseboilogieque.unblog.fr', (376,)),\n",
       " ('www.kostenlose-ausmalbilder.de', (377,)),\n",
       " ('www.usec-utc.fr', (378,)),\n",
       " ('bildungsklick.de', (379,)),\n",
       " ('www.mallinckrodt-gymnasium.de', (380,)),\n",
       " ('www.biologie-online.eu', (381,)),\n",
       " ('sousslayers.com', (382,)),\n",
       " ('www.infomysteres.com', (383,)),\n",
       " ('ssl.bing.com', (384,)),\n",
       " ('www.annuairedeforums.com', (385,)),\n",
       " ('www.lepoint.fr', (386,)),\n",
       " ('static1.shopoon.fr', (387,)),\n",
       " ('wwwimages.adobe.com', (388,)),\n",
       " ('www.adobe.com', (389,)),\n",
       " ('player.mediabong.com', (390,)),\n",
       " ('listener.ezakus.net', (391,)),\n",
       " ('mm.eulerian.net', (392,)),\n",
       " ('images.orangepublicite.fr', (393,)),\n",
       " ('static.videostep.com', (394,)),\n",
       " ('kweb.videostep.com', (395,)),\n",
       " ('u.videostep.com', (396,)),\n",
       " ('s.videostep.com', (397,)),\n",
       " ('i1-js-14-3-01-10334-522963629-i.init.cedexis-radar.net', (398,)),\n",
       " ('cloudfront.cedexis.com', (399,)),\n",
       " ('probe.cedexis.org', (400,)),\n",
       " ('val.cloudwatt.bench.cedexis.com', (401,)),\n",
       " ('us-east1.joyent.bench.cedexis.com', (402,)),\n",
       " ('fbe.linternaute.com', (403,)),\n",
       " ('i-cms.linternaute.com', (404,)),\n",
       " ('jnchaintreuil.com', (405,)),\n",
       " ('www.les-calculatrices.com', (406,)),\n",
       " ('www.clg-bosco.ac-aix-marseille.fr', (407,)),\n",
       " ('www.canal-u.tv', (408,)),\n",
       " ('www.ac-orleans-tours.fr', (409,)),\n",
       " ('www.clg-aragon.ac-aix-marseille.fr', (410,)),\n",
       " ('www.enpc.fr', (411,)),\n",
       " ('taha-base.voila.net', (412,)),\n",
       " ('cdn1.theo-et-mathilde.com', (413,)),\n",
       " ('blogs.crdp-limousin.fr', (414,)),\n",
       " ('www.devoir-de-philosophie.com', (415,)),\n",
       " ('popoblog.unblog.fr', (416,)),\n",
       " ('www.preinscriptions-uy1.uninet.cm', (417,)),\n",
       " ('xavier.hubaut.info', (418,)),\n",
       " ('www.annuaire-sciences.fr', (419,)),\n",
       " ('www.ilovegenerator.com', (420,)),\n",
       " ('coachingscolaire02.e-monsite.com', (421,)),\n",
       " ('www.memoirefacile.com', (422,)),\n",
       " ('www.lasouris-web.org', (423,)),\n",
       " ('dcf.linternaute.com', (424,)),\n",
       " ('cba.linternaute.com', (425,)),\n",
       " ('dfe.linternaute.com', (426,)),\n",
       " ('cec.linternaute.com', (427,)),\n",
       " ('cdf.linternaute.com', (428,)),\n",
       " ('bea.linternaute.com', (429,)),\n",
       " ('fde.linternaute.com', (430,)),\n",
       " ('ead.linternaute.com', (431,)),\n",
       " ('bbc.linternaute.com', (432,)),\n",
       " ('bdf.linternaute.com', (433,)),\n",
       " ('3karm2h0zl.s.ad6media.fr', (434,)),\n",
       " ('fba.linternaute.com', (435,)),\n",
       " ('fec.linternaute.com', (436,)),\n",
       " ('ceb.linternaute.com', (437,)),\n",
       " ('z-ecx.images-amazon.com', (438,)),\n",
       " ('fls-eu.amazon.fr', (439,)),\n",
       " ('g-ecx.images-amazon.com', (440,)),\n",
       " ('www.amazon.fr', (441,)),\n",
       " ('d3l3lkinz3f56t.cloudfront.net', (442,)),\n",
       " ('static.amazon.fr', (443,)),\n",
       " ('fls-eu.amazon.com', (444,)),\n",
       " ('df9v6ngw2bw7n.cloudfront.net', (445,)),\n",
       " ('www.algerie-dz.com', (446,)),\n",
       " ('www.cerimes.fr', (447,)),\n",
       " ('plugins.longtailvideo.com', (448,)),\n",
       " ('i1-js-14-3-01-10077-298020881-i.init.cedexis-radar.net', (449,)),\n",
       " ('fastlydsa.bench.cedexis.com', (450,)),\n",
       " ('akamai.cedexis.com', (451,)),\n",
       " ('www.lpthe.jussieu.fr', (452,)),\n",
       " ('communaute.lexpress.fr', (453,)),\n",
       " ('admin.brightcove.com', (454,)),\n",
       " ('static.lexpress.fr', (455,)),\n",
       " ('www.lexpress.fr', (456,)),\n",
       " ('static.mediabong.com', (457,)),\n",
       " ('korriban.mediabong.com', (458,)),\n",
       " ('codeorigin.jquery.com', (459,)),\n",
       " ('files.player.mediabong.com', (460,)),\n",
       " ('foglio.basilic.io', (461,)),\n",
       " ('video.mediabong.com', (462,)),\n",
       " ('i1-js-14-3-01-10608-847051798-i.init.cedexis-radar.net', (463,)),\n",
       " ('chinacache.cedexis.com', (464,)),\n",
       " ('aws-ap-southeast-2a.bench.cedexis.com', (465,)),\n",
       " ('fr-groupe01.videoplaza.tv', (466,)),\n",
       " ('liverail.api.peer39.net', (467,)),\n",
       " ('www.enigmes.net', (468,)),\n",
       " ('piwik.cndp.fr', (469,)),\n",
       " ('www.grioo.com', (470,)),\n",
       " ('serge.mehl.free.fr', (471,)),\n",
       " ('i1-js-14-3-01-11074-787764352-i.init.cedexis-radar.net', (472,)),\n",
       " ('limelight.cedexis.com', (473,)),\n",
       " ('ec2-us-west-1a.cedexis.com', (474,)),\n",
       " ('i1-js-14-3-01-10608-267342377-i.init.cedexis-radar.net', (475,)),\n",
       " ('edgecast-adn.cedexis.com', (476,)),\n",
       " ('onapp.cedexis.com', (477,)),\n",
       " ('bitgravity.cedexis.com', (478,)),\n",
       " ('www.lacosmo.com', (479,)),\n",
       " ('www.cite-sciences.fr', (480,)),\n",
       " ('www.universcience.fr', (481,)),\n",
       " ('home.web.cern.ch', (482,)),\n",
       " ('framework.web.cern.ch', (483,)),\n",
       " ('public.web.cern.ch', (484,)),\n",
       " ('cds.cern.ch', (485,)),\n",
       " ('lp.longtailvideo.com', (486,)),\n",
       " ('press.web.cern.ch', (487,)),\n",
       " ('cdsweb.cern.ch', (488,)),\n",
       " ('www.conspirovniscience.com', (489,)),\n",
       " ('i.creativecommons.org', (490,)),\n",
       " ('mediastream.cern.ch', (491,)),\n",
       " ('www.newscientist.com', (492,)),\n",
       " ('du8783wkf05yr.cloudfront.net', (493,)),\n",
       " ('metrics.reedbusiness.net', (494,)),\n",
       " ('www.sciencedaily.com', (495,)),\n",
       " ('images.sciencedaily.com', (496,)),\n",
       " ('res.cloudinary.com', (497,)),\n",
       " ('img.mit.edu', (498,)),\n",
       " ('web.mit.edu', (499,)),\n",
       " ('www.lefigaro.fr', (500,)),\n",
       " ('api.fidji.lefigaro.fr', (501,)),\n",
       " ('assets5.lefigaro.fr', (502,)),\n",
       " ('plus.lefigaro.fr', (503,)),\n",
       " ('assets3.lefigaro.fr', (504,)),\n",
       " ('assets2.lefigaro.fr', (505,)),\n",
       " ('assets4.lefigaro.fr', (506,)),\n",
       " ('assets6.lefigaro.fr', (507,)),\n",
       " ('assets1.lefigaro.fr', (508,)),\n",
       " ('assets-data.lefigaro.fr', (509,)),\n",
       " ('fr.ad4mat.net', (510,)),\n",
       " ('assets-pub.lefigaro.fr', (511,)),\n",
       " ('video.lefigaro.fr', (512,)),\n",
       " ('dnn506yrbagrg.cloudfront.net', (513,)),\n",
       " ('lefigaro.fr', (514,)),\n",
       " ('static.ad4mat.net', (515,)),\n",
       " ('platform.linkedin.com', (516,)),\n",
       " ('cyrillus.commander1.com', (517,)),\n",
       " ('www.ticketac.com', (518,)),\n",
       " ('media.cyrillus.fr', (519,)),\n",
       " ('i1-js-14-3-01-10036-547638031-i.init.cedexis-radar.net', (520,)),\n",
       " ('www.nasa.gov', (521,)),\n",
       " ('ams.nasa.gov', (522,)),\n",
       " ('physics.aps.org', (523,)),\n",
       " ('c.yousee.com', (524,)),\n",
       " ('images.gizmag.com', (525,)),\n",
       " ('files.gizmag.com', (526,)),\n",
       " ('edge.sharethis.com', (527,)),\n",
       " ('csm2waycm-atl.netmng.com', (528,)),\n",
       " ('indico.cern.ch', (529,)),\n",
       " ('irfu.cea.fr', (530,)),\n",
       " ('skeptics.stackexchange.com', (531,)),\n",
       " ('i.stack.imgur.com', (532,)),\n",
       " ('graph.facebook.com', (533,)),\n",
       " ('wiki.answers.com', (534,)),\n",
       " ('global-header.s3.amazonaws.com', (535,)),\n",
       " ('snip.answers.com', (536,)),\n",
       " ('answ-img.s3.amazonaws.com', (537,)),\n",
       " ('t.brand-server.com', (538,)),\n",
       " ('cm.adgrx.com', (539,)),\n",
       " ('casale-match.dotomi.com', (540,)),\n",
       " ('easystoregeo.widgetvillage.com', (541,)),\n",
       " ('dtm.gap.com', (542,)),\n",
       " ('ocsp2.globalsign.com', (543,)),\n",
       " ('stackauth.com', (544,)),\n",
       " ('vcm-match.dotomi.com', (545,)),\n",
       " ('rmx-match.dotomi.com', (546,)),\n",
       " ('brightroll-match.dotomi.com', (547,)),\n",
       " ('openx2-match.dotomi.com', (548,)),\n",
       " ('ajax.cloudflare.com', (549,)),\n",
       " ('lfhck.com', (550,)),\n",
       " ('www.reddit.com', (551,)),\n",
       " ('www.stumbleupon.com', (552,)),\n",
       " ('dsms0mj1bbhn4.cloudfront.net', (553,)),\n",
       " ('www.fromquarkstoquasars.com', (554,)),\n",
       " ('platform.stumbleupon.com', (555,)),\n",
       " ('platform.tumblr.com', (556,)),\n",
       " ('fromquarkstoquasars.disqus.com', (557,)),\n",
       " ('go.disqus.com', (558,)),\n",
       " ('disqus.com', (559,)),\n",
       " ('api.pinterest.com', (560,)),\n",
       " ('n19.hal9000.redintelligence.net', (561,)),\n",
       " ('n16.hal9000.redintelligence.net', (562,)),\n",
       " ('a.nonstoppartner.net', (563,)),\n",
       " ('banner.nonstoppartner.net', (564,)),\n",
       " ('badge.stumbleupon.com', (565,)),\n",
       " ('books.google.fr', (566,)),\n",
       " ('bks2.books.google.fr', (567,)),\n",
       " ('www.gravatar.com', (568,)),\n",
       " ('www.paypal.com', (569,)),\n",
       " ('plus.google.com', (570,)),\n",
       " ('g.courtial.free.fr', (571,)),\n",
       " ('pgj.pagesperso-orange.fr', (572,)),\n",
       " ('perso.wanadoo.fr', (573,)),\n",
       " ('pagesperso-orange.fr', (574,)),\n",
       " ('www.phy.olemiss.edu', (575,)),\n",
       " ('planete.gaia.free.fr', (576,)),\n",
       " ('cybergeo.revues.org', (577,)),\n",
       " ('static.openedition.org', (578,)),\n",
       " ('auth.openedition.org', (579,)),\n",
       " ('feynman.phy.ulaval.ca', (580,)),\n",
       " ('cel.archives-ouvertes.fr', (581,)),\n",
       " ('i1-js-14-3-01-10077-117585415-i.init.cedexis-radar.net', (582,)),\n",
       " ('board.fr.ogame.gameforge.com', (583,)),\n",
       " ('lartist.site.free.fr', (584,)),\n",
       " ('brazzo.comastuff.com', (585,)),\n",
       " ('board.ogame.fr', (586,)),\n",
       " ('i32.servimg.com', (587,)),\n",
       " ('image.board.gameforge.com', (588,)),\n",
       " ('statistics.board.gfsrv.net', (589,)),\n",
       " ('sciences.blogs.liberation.fr', (590,)),\n",
       " ('s1.libe.com', (591,)),\n",
       " ('www.typepad.com', (592,)),\n",
       " ('linkhelp.clients.google.com', (593,)),\n",
       " ('www.swissinfo.ch', (594,)),\n",
       " ('i1-js-14-3-01-12426-470619837-i.init.cedexis-radar.net', (595,)),\n",
       " ('cdx1.ecritel.bench.cedexis.com', (596,)),\n",
       " ('fastly.bench.cedexis.com.global.prod.fastly.net', (597,)),\n",
       " ('fastly.bench.cedexis.com', (598,)),\n",
       " ('www.enviscope.com', (599,)),\n",
       " ('phpmyvisites.taonix.net', (600,)),\n",
       " ('datayr.yves-rocher.fr', (601,)),\n",
       " ('www.unicef.org', (602,)),\n",
       " ('angelsanddemons.web.cern.ch', (603,)),\n",
       " ('static1.web.cern.ch', (604,)),\n",
       " ('i76.photobucket.com', (605,)),\n",
       " ('a137.idata.over-blog.com', (606,)),\n",
       " ('a397.idata.over-blog.com', (607,)),\n",
       " ('cui.unige.ch', (608,)),\n",
       " ('img.over-blog.com', (609,)),\n",
       " ('www.lantimatiere.info', (610,)),\n",
       " ('www.csnsm.in2p3.fr', (611,)),\n",
       " ('i2.wp.com', (612,)),\n",
       " ('www.youscribe.com', (613,)),\n",
       " ('img.uscri.be', (614,)),\n",
       " ('www.cernland.net', (615,)),\n",
       " ('docs.google.com', (616,)),\n",
       " ('gg.google.com', (617,)),\n",
       " ('47.docs.google.com', (618,)),\n",
       " ('80.docs.google.com', (619,)),\n",
       " ('www.nuklearforum.ch', (620,)),\n",
       " ('static.mxstatic.com', (621,)),\n",
       " ('www.maxisciences.com', (622,)),\n",
       " ('img0.mxstatic.com', (623,)),\n",
       " ('img1.mxstatic.com', (624,)),\n",
       " ('ocsp.comodoca.com', (625,)),\n",
       " ('www.eurekalert.org', (626,)),\n",
       " ('cool-antihydrogen.web.cern.ch', (627,)),\n",
       " ('live.ej.iop.org', (628,)),\n",
       " ('oas.iop.org', (629,)),\n",
       " ('www.nature.com', (630,)),\n",
       " ('cmsdesign.au.dk', (631,)),\n",
       " ('auinstallation28.cs.au.dk', (632,)),\n",
       " ('www.au.dk', (633,)),\n",
       " ('pure.au.dk', (634,)),\n",
       " ('www.physik.uzh.ch', (635,)),\n",
       " ('www.traqueur-stellaire.net', (636,)),\n",
       " ('flattr.com', (637,)),\n",
       " ('api.flattr.com', (638,)),\n",
       " ('static.addtoany.com', (639,)),\n",
       " ('www.pagerank.fr', (640,)),\n",
       " ('www.pieuvre.ca', (641,)),\n",
       " ('pieuvre.ca', (642,)),\n",
       " ('prophychi.free.fr', (643,)),\n",
       " ('l.longtailvideo.com', (644,)),\n",
       " ('lpsc.in2p3.fr', (645,)),\n",
       " ('www.nobelprize.org', (646,)),\n",
       " ('www.berkeley.edu', (647,)),\n",
       " ('3.bp.blogspot.com', (648,)),\n",
       " ('bigscience.web.cern.ch', (649,)),\n",
       " ('www.lbl.gov', (650,)),\n",
       " ('sciencereview.berkeley.edu', (651,)),\n",
       " ('4.bp.blogspot.com', (652,)),\n",
       " ('berkeleylabrecruiters.blogspot.fr', (653,)),\n",
       " ('berkeleylabrecruiters.blogspot.com', (654,)),\n",
       " ('2.bp.blogspot.com', (655,)),\n",
       " ('img.youtube.com', (656,)),\n",
       " ('widgets.twimg.com', (657,)),\n",
       " ('www.blogblog.com', (658,)),\n",
       " ('badge.facebook.com', (659,)),\n",
       " ('img2.blogblog.com', (660,)),\n",
       " ('img1.blogblog.com', (661,)),\n",
       " ('www.ig.gmodules.com', (662,)),\n",
       " ('swf.yowindow.com', (663,)),\n",
       " ('bnote.googlecode.com', (664,)),\n",
       " ('yowindow.com', (665,)),\n",
       " ('www.blogger.com', (666,)),\n",
       " ('commons.wikimedia.org', (667,)),\n",
       " ('cbks0.google.com', (668,)),\n",
       " ('feedburner.google.com', (669,)),\n",
       " ('feeds.feedburner.com', (670,)),\n",
       " ('eltamiz.com', (671,)),\n",
       " ('26.docs.google.com', (672,)),\n",
       " ('www.antiquarks.org', (673,)),\n",
       " ('s3.thingpic.com', (674,)),\n",
       " ('fr.wiktionary.org', (675,)),\n",
       " ('accounts.youtube.com', (676,)),\n",
       " ('drive.google.com', (677,)),\n",
       " ('0.drive.google.com', (678,)),\n",
       " ('accounts.google.fr', (679,)),\n",
       " ('quarks.lal.in2p3.fr', (680,)),\n",
       " ('fdata.over-blog.net', (681,)),\n",
       " ('services.supportduweb.com', (682,)),\n",
       " ('www.inexplique-endebat.com', (683,)),\n",
       " ('idata.over-blog.com', (684,)),\n",
       " ('a401.idata.over-blog.com', (685,)),\n",
       " ('a142.idata.over-blog.com', (686,)),\n",
       " ('a400.idata.over-blog.com', (687,)),\n",
       " ('a395.idata.over-blog.com', (688,)),\n",
       " ('a392.idata.over-blog.com', (689,)),\n",
       " ('a407.idata.over-blog.com', (690,)),\n",
       " ('a402.idata.over-blog.com', (691,)),\n",
       " ('a136.idata.over-blog.com', (692,)),\n",
       " ('a404.idata.over-blog.com', (693,)),\n",
       " ('external.wikio.fr', (694,)),\n",
       " ('fdata.over-blog.com', (695,)),\n",
       " ('a54.idata.over-blog.com', (696,)),\n",
       " ('a403.idata.over-blog.com', (697,)),\n",
       " ('a406.idata.over-blog.com', (698,)),\n",
       " ('a141.idata.over-blog.com', (699,)),\n",
       " ('a405.idata.over-blog.com', (700,)),\n",
       " ('a398.idata.over-blog.com', (701,)),\n",
       " ('a133.idata.over-blog.com', (702,)),\n",
       " ('www.anonymousdmp.com', (703,)),\n",
       " ('www.audienceinsights.net', (704,)),\n",
       " ('external.labs.ebuzzing.fr', (705,)),\n",
       " ('www.paypalobjects.com', (706,)),\n",
       " ('id.google.fr', (707,)),\n",
       " ('maverick.inria.fr', (708,)),\n",
       " ('www.masc.ulg.ac.be', (709,)),\n",
       " ('www.in2p3.fr', (710,)),\n",
       " ('i1-js-14-3-01-10077-733131946-i.init.cedexis-radar.net', (711,)),\n",
       " ('netdna.cedexis.com', (712,)),\n",
       " ('ecvideo.aufeminin.com', (713,)),\n",
       " ('ver-adt.vindicosuite.com', (714,)),\n",
       " ('c.vindicosuite.com', (715,)),\n",
       " ('b12.myspace.com', (716,)),\n",
       " ('integrate.factiva.com', (717,)),\n",
       " ('rr.office.microsoft.com', (718,)),\n",
       " ('services.highbeam.com', (719,)),\n",
       " ('www.worldlingo.com', (720,)),\n",
       " ('www.microsofttranslator.com', (721,)),\n",
       " ('bibliotheque.clermont-universite.fr', (722,)),\n",
       " ('imglib.lbl.gov', (723,)),\n",
       " ('www.webdam.com', (724,)),\n",
       " ('timeline.web.cern.ch', (725,)),\n",
       " ('www.acme.com', (726,)),\n",
       " ('i0.wp.com', (727,)),\n",
       " ('i1.wp.com', (728,)),\n",
       " ('www.sciencesaco.fr', (729,)),\n",
       " ('sciencesaco.fr', (730,)),\n",
       " ('www.sciences.univ-nantes.fr', (731,)),\n",
       " ('vcc.zih.tu-dresden.de', (732,)),\n",
       " ('translate.google.fr', (733,)),\n",
       " ('www.tpe-antimatiere-ste-anne.sitew.com', (734,)),\n",
       " ('www.sitew.com', (735,)),\n",
       " ('mfs1.sitew.org', (736,)),\n",
       " ('mfs3.sitew.org', (737,)),\n",
       " ('mfs0.sitew.org', (738,)),\n",
       " ('mfs2.sitew.org', (739,)),\n",
       " ('www.librairiepantoute.com', (740,)),\n",
       " ('assets.entrepotnumerique.com', (741,)),\n",
       " ('www.entrepotnumerique.com', (742,)),\n",
       " ('science-for-everyone.over-blog.com', (743,)),\n",
       " ('images.marmara.com', (744,)),\n",
       " ('it3yv1v73b.s.ad6media.fr', (745,)),\n",
       " ('ssl.sitew.org', (746,)),\n",
       " ('fr.openclassrooms.com', (747,)),\n",
       " ('static.siteduzero.com', (748,)),\n",
       " ('doug1izaerwt3.cloudfront.net', (749,)),\n",
       " ('user-disp.tidaltv.com', (750,)),\n",
       " ('cs.specificclick.net', (751,)),\n",
       " ('www.googleapis.com', (752,)),\n",
       " ('ocsp.godaddy.com', (753,)),\n",
       " ('i1-js-14-3-01-10077-479630930-i.init.cedexis-radar.net', (754,)),\n",
       " ('i1-js-14-3-01-10077-314473747-i.init.cedexis-radar.net', (755,)),\n",
       " ('level3.cedexis.com', (756,)),\n",
       " ('lejournal.cnrs.fr', (757,)),\n",
       " ('stats.lejournal.cnrs.fr', (758,)),\n",
       " ('ocsp.incommon.org', (759,)),\n",
       " ('www.princeton.edu', (760,)),\n",
       " ('www.britannica.com', (761,)),\n",
       " ('britannica.com', (762,)),\n",
       " ('www.merriam-webster.com', (763,)),\n",
       " ('www.int.washington.edu', (764,)),\n",
       " ('www.effiliation-hebergement.com', (765,)),\n",
       " ('www.savoirs.essonne.fr', (766,)),\n",
       " ('www.ntu.ac.uk', (767,)),\n",
       " ('gtssldv-ocsp.geotrust.com', (768,)),\n",
       " ('f.fontdeck.com', (769,)),\n",
       " ('search.ntu.ac.uk', (770,)),\n",
       " ('securelp.longtailvideo.com', (771,)),\n",
       " ('www.tela-botanica.org', (772,)),\n",
       " ('www.hotcoursesabroad.com', (773,)),\n",
       " ('clients6.google.com', (774,)),\n",
       " ('0.docs.google.com', (775,)),\n",
       " ('1.drive.google.com', (776,)),\n",
       " ('0.talkgadget.google.com', (777,)),\n",
       " ('www.ncbi.nlm.nih.gov', (778,)),\n",
       " ('static.pubmed.gov', (779,)),\n",
       " ('blast.ncbi.nlm.nih.gov', (780,)),\n",
       " ('www.iegallery.com', (781,)),\n",
       " ('annotathon.org', (782,)),\n",
       " ('annotathon.univ-mrs.fr', (783,)),\n",
       " ('javadl-esd-secure.oracle.com', (784,)),\n",
       " ('www.phylotree.org', (785,)),\n",
       " ('www.phylogeny.fr', (786,)),\n",
       " ('goo.gl', (787,)),\n",
       " ('picda.ilius.net', (788,)),\n",
       " ('spot.static.meetic.com', (789,)),\n",
       " ('img1.video.s-msn.com', (790,)),\n",
       " ('img2.video.s-msn.com', (791,)),\n",
       " ('hub.video.msn.com', (792,)),\n",
       " ('video.msn.com', (793,)),\n",
       " ('www.mail.live.com', (794,)),\n",
       " ('i.ssix.io', (795,)),\n",
       " ('login.secure.emea.msn.com', (796,)),\n",
       " ('mw2.google.com', (797,)),\n",
       " ('update.megasoftware.net', (798,)),\n",
       " ('www.megasoftware.net', (799,)),\n",
       " ('snippets-stats.mozilla.org', (800,)),\n",
       " ('shell.windows.com', (801,)),\n",
       " ('toolbar.google.fr', (802,)),\n",
       " ('translate.google.com', (803,)),\n",
       " ('clients3.google.com', (804,)),\n",
       " ('toolbar.google.com', (805,)),\n",
       " ('clients4.google.com', (806,)),\n",
       " ('mscrl.microsoft.com', (807,)),\n",
       " ('cache.pack.google.com', (808,)),\n",
       " ('r2---sn-gxo5uxg-jqbe.c.pack.google.com', (809,)),\n",
       " ('csc3-2010-crl.verisign.com', (810,)),\n",
       " ('crl.verisign.com', (811,)),\n",
       " ('mail.google.com', (812,)),\n",
       " ('toolbarqueries.clients.google.com', (813,)),\n",
       " ('clients2.google.com', (814,)),\n",
       " ('chatenabled.mail.google.com', (815,)),\n",
       " ('b.mail.google.com', (816,)),\n",
       " ('tools.google.com', (817,)),\n",
       " ('developer.android.com', (818,)),\n",
       " ('translate.googleapis.com', (819,)),\n",
       " ('armmf.adobe.com', (820,)),\n",
       " ('chateaut.free.fr', (821,)),\n",
       " ('www.f-iniciativas.fr', (822,)),\n",
       " ('e.visuels.poliris.com', (823,)),\n",
       " ('immobilier.lepoint.fr', (824,)),\n",
       " ('9.visuels.poliris.com', (825,)),\n",
       " ('f.visuels.poliris.com', (826,)),\n",
       " ('5.visuels.poliris.com', (827,)),\n",
       " ('4.visuels.poliris.com', (828,)),\n",
       " ('6.visuels.poliris.com', (829,)),\n",
       " ('news.google.fr', (830,)),\n",
       " ('www.capital.fr', (831,)),\n",
       " ('bdm.capital.fr', (832,)),\n",
       " ('emailretargeting.com', (833,)),\n",
       " ('dc1w54.wysistat.com', (834,)),\n",
       " ('static.cloud-media.fr', (835,)),\n",
       " ('www.ultimedia.com', (836,)),\n",
       " ('i.ytimg.com', (837,)),\n",
       " ('www.planet.fr', (838,)),\n",
       " ('preprod-img.planet.fr', (839,)),\n",
       " ('nb-commons.storage.googleapis.com', (840,)),\n",
       " ('img.planet.fr', (841,)),\n",
       " ('static.beead.net', (842,)),\n",
       " ('endor.mediabong.com', (843,)),\n",
       " ('static.polldaddy.com', (844,)),\n",
       " ('planet.purl.fr', (845,)),\n",
       " ('de.wikipedia.org', (846,)),\n",
       " ('gm1.ggpht.com', (847,)),\n",
       " ('www.altera.com', (848,)),\n",
       " ('altera.us', (849,)),\n",
       " ('www.uqac.ca', (850,)),\n",
       " ('fr.wikibooks.org', (851,)),\n",
       " ('www.sfr.fr', (852,)),\n",
       " ('static.s-sfr.fr', (853,)),\n",
       " ('metrics.sfr.fr', (854,)),\n",
       " ('deliv.leboncoin.fr', (855,)),\n",
       " ('com-sfr.netmng.com', (856,)),\n",
       " ('s7.s-sfr.fr', (857,)),\n",
       " ('s8.s-sfr.fr', (858,)),\n",
       " ('s4.s-sfr.fr', (859,)),\n",
       " ('s2.s-sfr.fr', (860,)),\n",
       " ('s1.s-sfr.fr', (861,)),\n",
       " ('s5.s-sfr.fr', (862,)),\n",
       " ('sfrmobile.inq.com', (863,)),\n",
       " ('as00.estara.com', (864,)),\n",
       " ('sfr.inq.com', (865,)),\n",
       " ('mediaeastv3.inq.com', (866,)),\n",
       " ('static.koreus.com', (867,)),\n",
       " ('thumbs.koreus.com', (868,)),\n",
       " ('o1fi67p86o.s.ad6media.fr', (869,)),\n",
       " ('www.koreus.com', (870,)),\n",
       " ('n32.hal9000.redintelligence.net', (871,)),\n",
       " ('n31.hal9000.redintelligence.net', (872,)),\n",
       " ('i3.ytimg.com', (873,)),\n",
       " ('r4---sn-gxo5uxg-jqbe.c.youtube.com', (874,)),\n",
       " ('embed.koreus.com', (875,)),\n",
       " ('i4.ytimg.com', (876,)),\n",
       " ('media.koreus.com', (877,)),\n",
       " ('reco.koreus.com', (878,)),\n",
       " ('r1---sn-gxo5uxg-jqbe.googlevideo.com', (879,)),\n",
       " ('g.microsoft.com', (880,)),\n",
       " ('r3---sn-gxo5uxg-jqbe.googlevideo.com', (881,)),\n",
       " ('r7---sn-cg07luee.googlevideo.com', (882,)),\n",
       " ('crl.geotrust.com', (883,)),\n",
       " ('pki.google.com', (884,)),\n",
       " ('go.intersil.com', (885,)),\n",
       " ('www.intersil.com', (886,)),\n",
       " ('storage.pardot.com', (887,)),\n",
       " ('brightcove.vo.llnwd.net', (888,)),\n",
       " ('www.autonomiste.com', (889,)),\n",
       " ('maps.google.com', (890,)),\n",
       " ('svrsecure-g3-crl.verisign.com', (891,)),\n",
       " ('as.blogbang.com', (892,)),\n",
       " ('www.blogbang.com', (893,)),\n",
       " ('crl3.digicert.com', (894,)),\n",
       " ('cdn2.blogbang.com', (895,)),\n",
       " ('maps.googleapis.com', (896,)),\n",
       " ('static.nrelate.com', (897,)),\n",
       " ('ieonlinews.microsoft.com', (898,)),\n",
       " ('radiospares-fr.rs-online.com', (899,)),\n",
       " ('img-europe.electrocomponents.com', (900,)),\n",
       " ('wa-europe.electrocomponents.com', (901,)),\n",
       " ('docs-europe.electrocomponents.com', (902,)),\n",
       " ('www.ferdinandpiette.com', (903,)),\n",
       " ('www.kicadlib.org', (904,)),\n",
       " ('pageperso.free.fr', (905,)),\n",
       " ('hal.inria.fr', (906,)),\n",
       " ('tel.archives-ouvertes.fr', (907,)),\n",
       " ('piwik-hal.ccsd.cnrs.fr', (908,)),\n",
       " ('arxiv.org', (909,)),\n",
       " ('static.arxiv.org', (910,)),\n",
       " ('arxiv-web-static1.s3.amazonaws.com', (911,)),\n",
       " ('www.pagesjaunes.fr', (912,)),\n",
       " ('logc258.at.pagesjaunes.fr', (913,)),\n",
       " ('ecn.api.tiles.virtualearth.net', (914,)),\n",
       " ('www.segulatechnologies.com', (915,)),\n",
       " ('www.segula.fr', (916,)),\n",
       " ('s2.static69.com', (917,)),\n",
       " ('s3.static69.com', (918,)),\n",
       " ('s1.static69.com', (919,)),\n",
       " ('www.rueducommerce.fr', (920,)),\n",
       " ('server1.counter.kameleoon.com', (921,)),\n",
       " ('static.rueducommerce.fr', (922,)),\n",
       " ('client.rueducommerce.fr', (923,)),\n",
       " ('server1.heatmap.kameleoon.com', (924,)),\n",
       " ('auth.rueducommerce.fr', (925,)),\n",
       " ('ea.rueducommerce.fr', (926,)),\n",
       " ('ws-rdc.web-boosting.net', (927,)),\n",
       " ('cart.rueducommerce.fr', (928,)),\n",
       " ('static.ak.connect.facebook.com', (929,)),\n",
       " ('www.sogep.com', (930,)),\n",
       " ('i1-js-14-3-01-10035-763099518-i.init.cedexis-radar.net', (931,)),\n",
       " ('akamai.static69.com', (932,)),\n",
       " ('cdnetworks.static69.com', (933,)),\n",
       " ('scholar.google.com', (934,)),\n",
       " ('js.socialike-me.com', (935,)),\n",
       " ('retargeting.veoxa.com', (936,)),\n",
       " ('img.publicidees.com', (937,)),\n",
       " ('ea.enviedefraises.fr', (938,)),\n",
       " ('runonce.msn.com', (939,)),\n",
       " ('maps.google.fr', (940,)),\n",
       " ('mts0.google.com', (941,)),\n",
       " ('mts1.google.com', (942,)),\n",
       " ('www.betclic.fr', (943,)),\n",
       " ('betclick.hs.llnwd.net', (944,)),\n",
       " ('www.webtide.com', (945,)),\n",
       " ('www.apache.org', (946,)),\n",
       " ('public.dhe.ibm.com', (947,)),\n",
       " ('download.oracle.com', (948,)),\n",
       " ('www.caucho.com', (949,)),\n",
       " ('jope.ow2.org', (950,)),\n",
       " ('download.eclipse.org', (951,)),\n",
       " ('master.dl.sourceforge.net', (952,)),\n",
       " ('download.jboss.org', (953,)),\n",
       " ('heanet.dl.sourceforge.net', (954,)),\n",
       " ('dfn.dl.sourceforge.net', (955,)),\n",
       " ('kent.dl.sourceforge.net', (956,)),\n",
       " ('optimate.dl.sourceforge.net', (957,)),\n",
       " ('freefr.dl.sourceforge.net', (958,)),\n",
       " ('docs.oracle.com', (959,)),\n",
       " ('compose.mail.yahoo.com', (960,)),\n",
       " ('login.yahoo.com', (961,)),\n",
       " ('www.megavoip.com', (962,)),\n",
       " ('www.cmmc.fr', (963,)),\n",
       " ('github.com', (964,)),\n",
       " ('github.global.ssl.fastly.net', (965,)),\n",
       " ('codekata.pragprog.com', (966,)),\n",
       " ('static.typepad.com', (967,)),\n",
       " ('up4.typepad.com', (968,)),\n",
       " ('lijit2waycm.netmng.com', (969,)),\n",
       " ('dtm.thingsremembered.com', (970,)),\n",
       " ('6a.typepad.com', (971,)),\n",
       " ('www.pierre.colomb.me', (972,)),\n",
       " ('javadl-esd.sun.com', (973,)),\n",
       " ('docs.postgresqlfr.org', (974,)),\n",
       " ('pcaboche.developpez.com', (975,)),\n",
       " ('zone.ni.com', (976,)),\n",
       " ('nsmetrics.ni.com', (977,)),\n",
       " ('www.ni.com', (978,)),\n",
       " ('dl.javafx.com', (979,)),\n",
       " ('armdl.adobe.com', (980,)),\n",
       " ('onlinestores.metaservices.microsoft.com', (981,)),\n",
       " ('www.gmail.com', (982,)),\n",
       " ('www.sonarqube.org', (983,)),\n",
       " ('repo1.maven.org', (984,)),\n",
       " ('dist.sonar.codehaus.org', (985,)),\n",
       " ('newsrss.bbc.co.uk', (986,)),\n",
       " ('fxfeeds.mozilla.com', (987,)),\n",
       " ('feeds.bbci.co.uk', (988,)),\n",
       " ('stackoverflow.com', (989,)),\n",
       " ('adzerk-www.s3.amazonaws.com', (990,)),\n",
       " ('markmail.org', (991,)),\n",
       " ('linsolas.developpez.com', (992,)),\n",
       " ('img.xooimage.com', (993,)),\n",
       " ('urpix.fr', (994,)),\n",
       " ('www.sonarfrance.com', (995,)),\n",
       " ('player.soundcloud.com', (996,)),\n",
       " ('img15.hostingpics.net', (997,)),\n",
       " ('img191.imageshack.us', (998,)),\n",
       " ('p1.soundcloud.com', (999,)),\n",
       " ('criteo.xooit.com', (1000,)),\n",
       " ...]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_site_freq = sorted(site_freq.items(), key=lambda x: x[1])\n",
    "sorted_site_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем разделить по пользованию skype, facebook и twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_features(csr, X):\n",
    "    features = [\n",
    "                'session_timespan', \n",
    "                'day_of_week', \n",
    "                'daily_aсtivity', \n",
    "                'freq_facebook', \n",
    "                'skype',\n",
    "                'twitter',\n",
    "               ]\n",
    "    sessions = X\n",
    "    sessions['user_id'] = np.zeros((len(X), ), dtype=int)\n",
    "    X_ext = prepare_train_set_fe(sessions,\n",
    "                                 site_freq_path=os.path.join(PATH_TO_DATA, 'site_freq.pkl'),\n",
    "                                 feature_names=features)\n",
    "    \n",
    "    csr_ext = csr.copy()\n",
    "    for t in np.unique(X_ext['daily_aсtivity']):\n",
    "        new_data = (X_ext['daily_aсtivity'] == t).astype('int')\n",
    "        csr_ext = hstack([csr_ext, new_data.to_numpy().reshape(-1, 1)])\n",
    "        \n",
    "    for t in np.unique(X_ext['day_of_week']):\n",
    "        new_data = (X_ext['day_of_week'] == t).astype('int')\n",
    "        csr_ext = hstack([csr_ext, new_data.to_numpy().reshape(-1, 1)])\n",
    "    \n",
    "    bm = [-1, 60, 5*60, 10*60, 30*60]\n",
    "    for i in range(len(bm) - 1):\n",
    "        new_data = ((X_ext['session_timespan'] > bm[i]) & \\\n",
    "                    (X_ext['session_timespan'] <= bm[i+1])).astype('int')\n",
    "        csr_ext = hstack([csr_ext, new_data.to_numpy().reshape(-1, 1)])\n",
    "        \n",
    "    csr_ext = hstack([csr_ext, X_ext['freq_facebook'].to_numpy().reshape(-1, 1)])\n",
    "    csr_ext = hstack([csr_ext, X_ext['skype'].to_numpy().reshape(-1, 1)])\n",
    "    csr_ext = hstack([csr_ext, X_ext['twitter'].to_numpy().reshape(-1, 1)])\n",
    "        \n",
    "    return csr_ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cv, X_test_cv = prepare_data_ext(train_df.iloc[:, :-1], test_df.iloc[:, :-1])\n",
    "train_share = int(.7 * len(train_df))\n",
    "X_train, y_train = X_train_cv.tocsr()[:train_share, :], train_df.target[:train_share]\n",
    "X_valid, y_valid  = X_train_cv.tocsr()[train_share:, :], train_df.target[train_share:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC for CountVectorizer + 6 feature with SGDClassifier is 0.96999\n"
     ]
    }
   ],
   "source": [
    "sgd_logit = SGDClassifier(loss='log', random_state=17, n_jobs=-1)\n",
    "sgd_logit.fit(X_train, y_train)\n",
    "logit_valid_pred_proba = sgd_logit.predict_proba(X_valid)\n",
    "roc_auc = roc_auc_score(y_valid, logit_valid_pred_proba[:, 1])\n",
    "print('ROC AUC for CountVectorizer + 6 feature with SGDClassifier is {:.5f}'.format(roc_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_logit = SGDClassifier(loss='log', random_state=17, n_jobs=-1)\n",
    "sgd_logit.fit(X_train_cv, train_df.target)\n",
    "logit_test_pred_proba = sgd_logit.predict_proba(X_test_cv)\n",
    "write_to_submission_file(logit_test_pred_proba[:, 1], os.path.join('answers', 'SGD_countVect_6fext.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получил Score = 0.94045. Ещё меньше. В общем пока оставляем вариант с 2 признаками и покрутим параметры самой модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_features(csr, X):\n",
    "    features = [\n",
    "                'day_of_week', \n",
    "                'daily_aсtivity',\n",
    "               ]\n",
    "    sessions = X\n",
    "    sessions['user_id'] = np.zeros((len(X), ), dtype=int)\n",
    "    X_ext = prepare_train_set_fe(sessions,\n",
    "                                 site_freq_path=os.path.join(PATH_TO_DATA, 'site_freq.pkl'),\n",
    "                                 feature_names=features)\n",
    "    \n",
    "    csr_ext = csr.copy()\n",
    "    for t in np.unique(X_ext['daily_aсtivity']):\n",
    "        new_data = (X_ext['daily_aсtivity'] == t).astype('int')\n",
    "        csr_ext = hstack([csr_ext, new_data.to_numpy().reshape(-1, 1)])\n",
    "        \n",
    "    for t in np.unique(X_ext['day_of_week']):\n",
    "        new_data = (X_ext['day_of_week'] == t).astype('int')\n",
    "        csr_ext = hstack([csr_ext, new_data.to_numpy().reshape(-1, 1)])\n",
    "        \n",
    "    return csr_ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cv, X_test_cv = prepare_data_ext(X_train_df, X_valid_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем логистическую регрессию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.17 s, sys: 1.13 s, total: 2.3 s\n",
      "Wall time: 14.5 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegressionCV(Cs=array([ 5.        ,  5.26315789,  5.52631579,  5.78947368,  6.05263158,\n",
       "        6.31578947,  6.57894737,  6.84210526,  7.10526316,  7.36842105,\n",
       "        7.63157895,  7.89473684,  8.15789474,  8.42105263,  8.68421053,\n",
       "        8.94736842,  9.21052632,  9.47368421,  9.73684211, 10.        ]),\n",
       "                     class_weight=None,\n",
       "                     cv=StratifiedKFold(n_splits=3, random_state=17, shuffle=True),\n",
       "                     dual=False, fit_intercept=True, intercept_scaling=1.0,\n",
       "                     l1_ratios=None, max_iter=100, multi_class='warn',\n",
       "                     n_jobs=-1, penalty='l2', random_state=17, refit=True,\n",
       "                     scoring=None, solver='lbfgs', tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "logit_c_values = np.linspace(5, 10, 20)\n",
    "logit_grid_searcher = LogisticRegressionCV(logit_c_values, random_state=17, cv=skf, n_jobs=-1)\n",
    "logit_grid_searcher.fit(X_train_cv, y_train_ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best mean score is 0.96453 correspond to C=7.631578947368421 with index 10\n"
     ]
    }
   ],
   "source": [
    "logit_cv_scores = list(logit_grid_searcher.scores_.values())[0]\n",
    "logit_mean_cv_scores = logit_cv_scores.mean(axis=0)\n",
    "assert (logit_grid_searcher.Cs_ == logit_c_values).all()\n",
    "i = np.argmax(logit_mean_cv_scores)\n",
    "accu_train = logit_mean_cv_scores[i]\n",
    "print('best mean score is {:.5f} correspond to C={} with index {}'.format(logit_mean_cv_scores[i], logit_c_values[i], i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Не похоже на то, чтобы что-то улучшилось. Попробуем ещё опорные вектора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 493 ms, sys: 65.1 ms, total: 558 ms\n",
      "Wall time: 9.21 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=3, random_state=17, shuffle=True),\n",
       "             error_score='raise-deprecating',\n",
       "             estimator=LinearSVC(C=1.0, class_weight=None, dual=True,\n",
       "                                 fit_intercept=True, intercept_scaling=1,\n",
       "                                 loss='squared_hinge', max_iter=1000,\n",
       "                                 multi_class='ovr', penalty='l2',\n",
       "                                 random_state=17, tol=0.0001, verbose=0),\n",
       "             iid='warn', n_jobs=-1,\n",
       "             param_grid={'C': array([0.01...\n",
       "       0.18068966, 0.21482759, 0.24896552, 0.28310345, 0.31724138,\n",
       "       0.35137931, 0.38551724, 0.41965517, 0.4537931 , 0.48793103,\n",
       "       0.52206897, 0.5562069 , 0.59034483, 0.62448276, 0.65862069,\n",
       "       0.69275862, 0.72689655, 0.76103448, 0.79517241, 0.82931034,\n",
       "       0.86344828, 0.89758621, 0.93172414, 0.96586207, 1.        ])},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "svm = LinearSVC(random_state=17)\n",
    "svm_params = {'C': np.linspace(1e-2, 1, 30)}\n",
    "\n",
    "svm_grid_searcher = GridSearchCV(svm, svm_params, n_jobs=-1, cv=skf, return_train_score=True)\n",
    "svm_grid_searcher.fit(X_train_cv, y_train_ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score is 0.96388 correspond to C=0.14655172413793105 with index 4\n"
     ]
    }
   ],
   "source": [
    "i = np.argmax(svm_grid_searcher.cv_results_['mean_test_score'])\n",
    "accu_train = svm_grid_searcher.cv_results_['mean_test_score'][i]\n",
    "print('best score is {:.5f} correspond to C={} with index {}'.format(accu_train, svm_grid_searcher.cv_results_['param_C'][i], i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ещё хуже, попробуем на полной выборке логистическую регрессию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cv, X_test_cv = prepare_data_ext(train_df.iloc[:, :-1], test_df.iloc[:, :-1])\n",
    "train_share = int(.7 * len(train_df))\n",
    "X_train, y_train = X_train_cv.tocsr()[:train_share, :], train_df.target[:train_share]\n",
    "X_valid, y_valid  = X_train_cv.tocsr()[train_share:, :], train_df.target[train_share:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC for CountVectorizer + 2 feature with LogisticRegression is 0.96854\n"
     ]
    }
   ],
   "source": [
    "logit = LogisticRegression(random_state=17, C=7.6316, n_jobs=-1)\n",
    "logit.fit(X_train, y_train)\n",
    "logit_valid_pred_proba = logit.predict_proba(X_valid)\n",
    "roc_auc = roc_auc_score(y_valid, logit_valid_pred_proba[:, 1])\n",
    "print('ROC AUC for CountVectorizer + 2 feature with LogisticRegression is {:.5f}'.format(roc_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Да, действительно всё только хуже"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Критерии оценки работы (только для Peer Review в специализации):\n",
    "- Правильные ли получились размерности матриц в п. 1? (max. 2 балла)\n",
    "- Правильным ли получилось значения ROC AUC в п. 2? (max. 4 балла)\n",
    "- Побит ли бенчмарк \"sgd_logit_benchmark.csv\" на публичной части рейтинга в соревновании Kaggle? (max. 2 балла)\n",
    "- Побит ли бенчмарк \"Logit +3 features\" на публичной части рейтинга в соревновании Kaggle? (max. 2 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Пути улучшения\n",
    "На этой неделе дается много времени на соревнование. Не забывайте вносить хорошие идеи, к которым Вы пришли по ходу соревнования, в описание финального проекта (`html`, `pdf` или `ipynb`). Это только в случае, если вы проходите специализацию.\n",
    "Что можно попробовать:\n",
    " - Использовать ранее построенные признаки для улучшения модели (проверить их можно на меньшей выборке по 150 пользователям, отделив одного из пользователей от остальных – это быстрее)\n",
    " - Настроить параметры моделей (например, коэффициенты регуляризации)\n",
    " - Если позволяют мощности (или хватает терпения), можно попробовать смешивание (блендинг) ответов бустинга и линейной модели. [Вот](http://mlwave.com/kaggle-ensembling-guide/) один из самых известных тьюториалов по смешиванию ответов алгоритмов, также хороша [статья](https://alexanderdyakonov.wordpress.com/2017/03/10/cтекинг-stacking-и-блендинг-blending) Александра Дьяконова\n",
    " - Обратите внимание, что в соревновании также даны исходные данные о посещенных веб-страницах Элис и остальными 1557 пользователями (*train.zip*). По этим данным можно сформировать свою обучающую выборку. \n",
    "\n",
    "На 6 неделе мы пройдем большой тьюториал по Vowpal Wabbit и попробуем его в деле, на данных соревнования."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
